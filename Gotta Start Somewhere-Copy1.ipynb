{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JayMichChi Part 2 Capstone\n",
    "### Goal: Jays summary notebook outlining your project's goals, methods, models, and EDA.\n",
    "\n",
    "This will be an overview of my approach with a well-articulated summary that includes your problem statement, outlines your proposed methods and models, defines any risks & assumptions, and includes any revisions from your initial goals & criteria, as needed.\n",
    "\n",
    "In addition, I will explain how I intend to store my data. \n",
    "\n",
    "You'll want to describe and create a data dictionary for your dataset(s), and perform your initial exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My Initial Goals\n",
    "My capstone takes data from the National Centers for Environmental Information, particularly information from 1996 to 2013, and will look at unusual storm occurrences and unusual weather phenomena, and mores specifically, information on storm paths, deaths, injuries, and property damage. \n",
    "\n",
    "### My hypothesis is:\n",
    "Can we reasonably predict the amount of annual deaths due to certain types of storm occurances based on past data?\n",
    "\n",
    "#### Information about the data:\n",
    "National Centers for Environmental Information (NCEI) is responsible for hosting and providing access to one of the most significant archives on Earth, with comprehensive oceanic, atmospheric, and geophysical data. From the depths of the ocean to the surface of the sun and from million-year-old sediment records to near real-time satellite images, NCEI is the Nation’s leading authority for environmental information.  It is from this source that I will primarily ingest data on the following: https://www.ncdc.noaa.gov/stormevents/\n",
    "\n",
    "### Storm Events Database\n",
    "The Storm Events Database contains a chronological listing, by states, of storm occurrences and unusual weather phenomena. Reports contain information on storm paths, deaths, injuries, and property damage.s\n",
    "\n",
    "The Storm Events Database currently contains data from January 1950 to August 2016, as entered by NOAA's National Weather Service (NWS). Due to changes in the data collection and processing procedures over time, there are unique periods of record available depending on the event type. I have chosen to look at the data from 1996 to 2013.\n",
    "\n",
    "### Data base\n",
    "I was able to find the data on the following NCDC website FTP server. \n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/\n",
    "\n",
    "    Index of /pub/data/swdi/stormevents/csvfiles/legacy/\n",
    "    Name\tSize\tDate Modified\n",
    "    ugc_areas.csv\t902 kB\t2/18/14, 12:00:00 AM\n",
    "    stormdata_2013.csv\t53.1 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2012.csv\t64.3 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2011.csv\t82.3 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2010.csv\t62.5 MB\t1/31/14, 12:00:00 AM\n",
    "    Stormdata_2009.csv\t49.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2008.csv\t60.6 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2007.csv\t49.7 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2006.csv\t26.0 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2005.csv\t34.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2004.csv\t33.1 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2003.csv\t32.1 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2002.csv\t32.0 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2001.csv\t30.6 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2000.csv\t35.7 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_1999.csv\t34.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_1998.csv\t37.8 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_1997.csv\t25.0 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_1996.csv\t22.9 MB\t9/19/13, 12:00:00 AM\n",
    "    [parent directory]\t\t\n",
    "\n",
    "\n",
    "I have successfully downloaded all 18 CSV files. My work will show that i merged the data via pandas concant processes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "\n",
    "Your work must:\n",
    "\n",
    "- Articulate “Specific aim”\n",
    "\n",
    "My goal DONE\n",
    "\n",
    "- Outline proposed methods and models\n",
    "\n",
    "Proposed methods are gathering the data, cleaning the date, fitting it to a few modles, TTS, run/score/graph\n",
    "\n",
    "Logistic Regression /  Linear Regression\n",
    "\n",
    "- Define risks & assumptions\n",
    "\n",
    "Annual events: need to be carefully \n",
    "confounding factors effect costings, fatalities that may not be completely repesented in the weather realated incident. National Guard, federal funding\n",
    "\n",
    "- Revise initial goals & success criteria, as needed\n",
    "\n",
    "- Create local database\n",
    "DONE on DBX\n",
    "- Describe data cleaning/munging techniques\n",
    "\n",
    "No Probe\n",
    "\n",
    "- Create a data dictionary\n",
    "\n",
    "summary explanation of each of the features\n",
    "\n",
    "- Perform & summarize EDA\n",
    "\n",
    "exploratory data analysis summary\n",
    "\n",
    "- ***Bonus:***\n",
    " - Explain how you intend to evaluate your results. What tuning metric and evaluation approaches do you intend to use?\n",
    " \n",
    " clean fit run models, look at the results... \n",
    " linear: error RSM differential \n",
    " logistic ;  classification accuracy score, confusion matrix\n",
    "coorelation number \n",
    " - Identify 1-2 additional datasets that may help you triangulate your findings. How might these relate to your data?\n",
    "aircraft incident, NTSB site\n",
    "Crop damage, crop price website\n",
    "\n",
    "- Create a blog post of at least 500 words (and 1-2 graphics!) that describes your assumptions and processes for EDA. Link to it in your Jupyter notebook.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### Necessary Deliverables / Submission\n",
    "\n",
    " - Materials must be submitted in a clearly labeled Jupyter notebook, including:\n",
    "  - Markdown writeups, code, and visualizations\n",
    " - Create your own repository under your user name, submit a link to that repository with your findings.\n",
    "\n",
    "---\n",
    "\n",
    "### Suggested Ways to Get Started\n",
    "\n",
    "- Don’t hesitate to write throwaway code to solve short term problems\n",
    "- Read the docs for whatever technologies you use. Most of the time, there is a tutorial that you can follow, but not always, and learning to read documentation is crucial to your success!\n",
    "- Write pseudocode before you write actual code. Thinking through the logic of something helps.  \n",
    "- Document **everything**.\n",
    "\n",
    "\n",
    "### Useful Resources\n",
    "\n",
    "- [Exploratory Data Analysis](http://insightdatascience.com/blog/eda-and-graphics-eli-bressert.html)\n",
    "- [Best practices for data documentation](https://www.dataone.org/all-best-practices)\n",
    "\n",
    "---\n",
    "\n",
    "### Project Feedback + Evaluation\n",
    "\n",
    "Your instructors will score each of your technical requirements using the scale below:\n",
    "\n",
    "    Score | Expectations\n",
    "    ----- | ------------\n",
    "    **0** | _Incomplete._\n",
    "    **1** | _Does not meet expectations._\n",
    "    **2** | _Meets expectations, good job!_\n",
    "    **3** | _Exceeds expectations, you wonderful creature, you!_\n",
    "\n",
    "This will serve as a helpful overall gauge of whether you met the project goals, but __the more important scores are the individual ones__ above, which can help you identify where to focus your efforts for the next project!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# loading dataset\n",
    "\n",
    "\n",
    "# import pandas as pd\n",
    "\n",
    "# df = pd.read_csv('ncdc_storm_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_1996.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_1997.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_1998.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_1999.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2000.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2001.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2002.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2003.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2004.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2005.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2006.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2007.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2008.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2009.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#here is my scroll. it works, but the database url_base changes in y2010, \n",
    "# so i will re-run this with the updated base_url\n",
    "\n",
    "import time\n",
    "url_list = ['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_1996.csv']\n",
    "web_count= 1996\n",
    "for pages in range(1,14):\n",
    "    web_count= 1 + web_count\n",
    "    count_update=str(web_count)\n",
    "    base_url = 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_' + count_update + '.csv'\n",
    "    url_list.append(base_url)\n",
    "url_list\n",
    "# This works wonderfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is my one time function that will load, append\n",
    "# for url in url_list:\n",
    "data1 = []\n",
    "for url in url_list:\n",
    "    page = pd.read_csv((url)) \n",
    "    data1.append(page)\n",
    "    time.sleep(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_all = pd.concat(data1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       BEGIN_YEARMONTH      BEGIN_DAY     BEGIN_TIME  END_YEARMONTH  \\\n",
      "count    735761.000000  735761.000000  735761.000000  735761.000000   \n",
      "mean     200293.408003      14.834721    1302.911895  200293.411558   \n",
      "std         405.907281       9.056114     676.908482     405.909385   \n",
      "min      199601.000000       1.000000       0.000000  199601.000000   \n",
      "25%      199910.000000       7.000000     800.000000  199910.000000   \n",
      "50%      200306.000000      14.000000    1500.000000  200306.000000   \n",
      "75%      200702.000000      23.000000    1815.000000  200702.000000   \n",
      "max      200912.000000      31.000000    2359.000000  200912.000000   \n",
      "\n",
      "             END_DAY       END_TIME    EPISODE_ID      EVENT_ID  \\\n",
      "count  735761.000000  735761.000000  7.357610e+05  7.357610e+05   \n",
      "mean       16.189741    1478.744905  2.695585e+05  4.080606e+06   \n",
      "std         9.083053     612.905340  3.698866e+05  2.332925e+06   \n",
      "min         1.000000       0.000000  1.000000e+00  3.419000e+03   \n",
      "25%         8.000000    1130.000000  3.283400e+04  2.029460e+05   \n",
      "50%        16.000000    1610.000000  1.341000e+05  5.355336e+06   \n",
      "75%        24.000000    1915.000000  1.996580e+05  5.539300e+06   \n",
      "max        31.000000    2359.000000  1.152422e+06  5.725449e+06   \n",
      "\n",
      "          STATE_FIPS           YEAR      ...           BEGIN_LAT  \\\n",
      "count  735760.000000  735761.000000      ...       406500.000000   \n",
      "mean       31.241076    2002.874828      ...           37.946366   \n",
      "std        16.943521       4.058276      ...            4.800925   \n",
      "min         1.000000    1996.000000      ...          -14.330000   \n",
      "25%              NaN    1999.000000      ...                 NaN   \n",
      "50%              NaN    2003.000000      ...                 NaN   \n",
      "75%              NaN    2007.000000      ...                 NaN   \n",
      "max        99.000000    2009.000000      ...           97.100000   \n",
      "\n",
      "           BEGIN_LON        END_LAT        END_LON  LAST_MOD_DATE  \\\n",
      "count  406501.000000  312950.000000  312951.000000   1.880990e+05   \n",
      "mean      -53.896385      38.272949     -42.906190   4.589732e+07   \n",
      "std        93.318673      11.866140     103.629673   3.737215e+07   \n",
      "min      -815.100000     -14.400000    -815.100000   2.007011e+07   \n",
      "25%              NaN            NaN            NaN            NaN   \n",
      "50%              NaN            NaN            NaN            NaN   \n",
      "75%              NaN            NaN            NaN            NaN   \n",
      "max       159.720000     483.100000     815.100000   1.000000e+08   \n",
      "\n",
      "       LAST_MOD_TIME  LAST_CERT_DATE  LAST_CERT_TIME   ADDCORR_FLG  \\\n",
      "count  188099.000000    1.880990e+05   188099.000000  25205.000000   \n",
      "mean     4177.796389    2.008357e+07     1333.171543      1.057766   \n",
      "std      4044.049121    8.164348e+03      396.566932      0.233306   \n",
      "min         0.000000    2.007021e+07        0.000000      1.000000   \n",
      "25%              NaN             NaN             NaN           NaN   \n",
      "50%              NaN             NaN             NaN           NaN   \n",
      "75%              NaN             NaN             NaN           NaN   \n",
      "max      9999.000000    2.011113e+07     2357.000000      2.000000   \n",
      "\n",
      "       ADDCORR_DATE  \n",
      "count  2.520500e+04  \n",
      "mean   2.009067e+07  \n",
      "std    1.897319e+03  \n",
      "min    2.009041e+07  \n",
      "25%             NaN  \n",
      "50%             NaN  \n",
      "75%             NaN  \n",
      "max    2.011122e+07  \n",
      "\n",
      "[8 rows x 32 columns]\n"
     ]
    }
   ],
   "source": [
    "print data1_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2010.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2011.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2012.csv',\n",
       " 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2013.csv']"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#as above\n",
    "# so i will re-run this with the updated base_url\n",
    "\n",
    "import time\n",
    "url_list_2 = ['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2010.csv']\n",
    "web_count= 2010\n",
    "for pages in range(1,4):\n",
    "    web_count= 1 + web_count\n",
    "    count_update=str(web_count)\n",
    "    base_url_2 = 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_' + count_update + '.csv'\n",
    "    url_list_2.append(base_url_2)\n",
    "url_list_2\n",
    "# This works wonderfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "URLError",
     "evalue": "<urlopen error ftp error: 530 Maximum number of connections exceeded.>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mURLError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-137-f7c678a1c8e8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0murl\u001b[0m \u001b[0;32min\u001b[0m \u001b[0murl_list_2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mpage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mdata2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m30.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    299\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[1;32m    300\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 301\u001b[0;31m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[1;32m    302\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[1;32m    303\u001b[0m                            else compression)\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/common.pyc\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    307\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 308\u001b[0;31m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_urlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    309\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'infer'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m             \u001b[0mcontent_encoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Content-Encoding'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36murlopen\u001b[0;34m(url, data, timeout, cafile, capath, cadefault, context)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0mopener\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_opener\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mopener\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0minstall_opener\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopener\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mopen\u001b[0;34m(self, fullurl, data, timeout)\u001b[0m\n\u001b[1;32m    427\u001b[0m             \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m         \u001b[0;31m# post-process response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, req, data)\u001b[0m\n\u001b[1;32m    445\u001b[0m         \u001b[0mprotocol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m         result = self._call_chain(self.handle_open, protocol, protocol +\n\u001b[0;32m--> 447\u001b[0;31m                                   '_open', req)\n\u001b[0m\u001b[1;32m    448\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    449\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36m_call_chain\u001b[0;34m(self, chain, kind, meth_name, *args)\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mfunc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeth_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mftp_open\u001b[0;34m(self, req)\u001b[0m\n\u001b[1;32m   1411\u001b[0m             \u001b[0mdirs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1412\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1413\u001b[0;31m             \u001b[0mfw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1414\u001b[0m             \u001b[0mtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'I'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m'D'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1415\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mattr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib2.pyc\u001b[0m in \u001b[0;36mconnect_ftp\u001b[0;34m(self, user, passwd, host, port, dirs, timeout)\u001b[0m\n\u001b[1;32m   1433\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconnect_ftp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpasswd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1434\u001b[0m         fw = ftpwrapper(user, passwd, host, port, dirs, timeout,\n\u001b[0;32m-> 1435\u001b[0;31m                         persistent=False)\n\u001b[0m\u001b[1;32m   1436\u001b[0m \u001b[0;31m##        fw.ftp.set_debuglevel(1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1437\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, user, passwd, host, port, dirs, timeout, persistent)\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeepalive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36minit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    884\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbusy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    885\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mftplib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFTP\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 886\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconnect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhost\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    887\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mftp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpasswd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0m_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdirs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36mconnect\u001b[0;34m(self, host, port, timeout)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfamily\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwelcome\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetresp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwelcome\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/ftplib.pyc\u001b[0m in \u001b[0;36mgetresp\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    222\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0merror_temp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mc\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'5'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_perm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0merror_proto\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mURLError\u001b[0m: <urlopen error ftp error: 530 Maximum number of connections exceeded.>"
     ]
    }
   ],
   "source": [
    "# This is my one time function that will load, append\n",
    "# for url in url_list:\n",
    "data2 = []\n",
    "for url in url_list_2:\n",
    "    page = pd.read_csv((url)) \n",
    "    data2.append(page)\n",
    "    time.sleep(30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2_all=pd.concat(data2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62795, 58)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([data1_all, data2_all], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(798556, 58)"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'BEGIN_YEARMONTH', u'BEGIN_DAY', u'BEGIN_TIME', u'END_YEARMONTH',\n",
      "       u'END_DAY', u'END_TIME', u'EPISODE_ID', u'EVENT_ID', u'STATE',\n",
      "       u'STATE_FIPS', u'YEAR', u'MONTH_NAME', u'EVENT_TYPE', u'CZ_TYPE',\n",
      "       u'CZ_FIPS', u'CZ_NAME', u'WFO', u'BEGIN_DATE_TIME', u'CZ_TIMEZONE',\n",
      "       u'END_DATE_TIME', u'INJURIES_DIRECT', u'INJURIES_INDIRECT',\n",
      "       u'DEATHS_DIRECT', u'DEATHS_INDIRECT', u'DAMAGE_PROPERTY',\n",
      "       u'DAMAGE_CROPS', u'SOURCE', u'MAGNITUDE', u'MAGNITUDE_TYPE',\n",
      "       u'FLOOD_CAUSE', u'CATEGORY', u'TOR_F_SCALE', u'TOR_LENGTH',\n",
      "       u'TOR_WIDTH', u'TOR_OTHER_WFO', u'TOR_OTHER_CZ_STATE',\n",
      "       u'TOR_OTHER_CZ_FIPS', u'TOR_OTHER_CZ_NAME', u'BEGIN_RANGE',\n",
      "       u'BEGIN_AZIMUTH', u'BEGIN_LOCATION', u'END_RANGE', u'END_AZIMUTH',\n",
      "       u'END_LOCATION', u'BEGIN_LAT', u'BEGIN_LON', u'END_LAT', u'END_LON',\n",
      "       u'EPISODE_NARRATIVE', u'EVENT_NARRATIVE', u'LAST_MOD_DATE',\n",
      "       u'LAST_MOD_TIME', u'LAST_CERT_DATE', u'LAST_CERT_TIME', u'LAST_MOD',\n",
      "       u'LAST_CERT', u'ADDCORR_FLG', u'ADDCORR_DATE'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert dataframe to csv\n",
    "\n",
    "df_all.to_csv('jm_cap.csv')\n",
    "#This worked!  Thank God this was successful on Friday night!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "IOError",
     "evalue": "File jm_cap.csv does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-ba0874032fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'jm_cap.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    560\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    561\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mchunksize\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    643\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 645\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    646\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    647\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m    797\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 799\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    800\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    801\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/io/parsers.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1211\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'allow_leading_cols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex_col\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1213\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_parser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1215\u001b[0m         \u001b[0;31m# XXX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader.__cinit__ (pandas/parser.c:3427)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/parser.pyx\u001b[0m in \u001b[0;36mpandas.parser.TextReader._setup_parser_source (pandas/parser.c:6861)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mIOError\u001b[0m: File jm_cap.csv does not exist"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('jm_cap.csv')\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(data1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.EVTYPE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
