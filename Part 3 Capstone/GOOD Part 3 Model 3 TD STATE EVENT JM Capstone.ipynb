{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sbn\n",
    "from patsy import dmatrices\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "plt.style.use('seaborn-white')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('../Part 2 Capstone/df2.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['MONTH_NAME'] = df1['MONTH_NAME'].apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "event_types = ['Drought' ,'Flood' ,'Winter Storm', 'OTHER', 'High Wind' ,'ThunderStorm',\n",
    " 'Wildfire', 'Rip Current' ,'Tropical Storm' ,'Tornado' ,'Hurricane', 'Tsunami',\n",
    " 'Landslide']\n",
    "for i in event_types:\n",
    "     df1['is_'+i] = df1['EVENT_TYPE'].apply(lambda x: 1 if i in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([nan, 'F1', 'F0', 'F2', 'F3', 'F4', 'EF3', 'EF1', 'EF0', 'EF2',\n",
       "       'EF4', 'EF5'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.TOR_F_SCALE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tor_all = pd.get_dummies(df1.TOR_F_SCALE, prefix=\"is\", prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False)\n",
    "tor_all_1 = pd.get_dummies(df1.STATE, prefix=\"is\", prefix_sep='_', dummy_na=False, columns=None, sparse=False, drop_first=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>is_ALABAMA</th>\n",
       "      <th>is_ALASKA</th>\n",
       "      <th>is_AMERICAN SAMOA</th>\n",
       "      <th>is_ARIZONA</th>\n",
       "      <th>is_ARKANSAS</th>\n",
       "      <th>is_ATLANTIC NORTH</th>\n",
       "      <th>is_ATLANTIC SOUTH</th>\n",
       "      <th>is_CALIFORNIA</th>\n",
       "      <th>is_COLORADO</th>\n",
       "      <th>is_CONNECTICUT</th>\n",
       "      <th>...</th>\n",
       "      <th>is_TENNESSEE</th>\n",
       "      <th>is_TEXAS</th>\n",
       "      <th>is_UTAH</th>\n",
       "      <th>is_VERMONT</th>\n",
       "      <th>is_VIRGIN ISLANDS</th>\n",
       "      <th>is_VIRGINIA</th>\n",
       "      <th>is_WASHINGTON</th>\n",
       "      <th>is_WEST VIRGINIA</th>\n",
       "      <th>is_WISCONSIN</th>\n",
       "      <th>is_WYOMING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   is_ALABAMA  is_ALASKA  is_AMERICAN SAMOA  is_ARIZONA  is_ARKANSAS  \\\n",
       "0         0.0        0.0                0.0         0.0          0.0   \n",
       "1         0.0        0.0                0.0         0.0          0.0   \n",
       "2         0.0        0.0                0.0         0.0          0.0   \n",
       "3         0.0        0.0                0.0         0.0          0.0   \n",
       "4         0.0        0.0                0.0         0.0          0.0   \n",
       "\n",
       "   is_ATLANTIC NORTH  is_ATLANTIC SOUTH  is_CALIFORNIA  is_COLORADO  \\\n",
       "0                0.0                0.0            0.0          1.0   \n",
       "1                0.0                0.0            0.0          0.0   \n",
       "2                0.0                0.0            0.0          0.0   \n",
       "3                0.0                0.0            0.0          0.0   \n",
       "4                0.0                0.0            0.0          0.0   \n",
       "\n",
       "   is_CONNECTICUT     ...      is_TENNESSEE  is_TEXAS  is_UTAH  is_VERMONT  \\\n",
       "0             0.0     ...               0.0       0.0      0.0         0.0   \n",
       "1             0.0     ...               0.0       1.0      0.0         0.0   \n",
       "2             0.0     ...               0.0       1.0      0.0         0.0   \n",
       "3             0.0     ...               0.0       1.0      0.0         0.0   \n",
       "4             0.0     ...               0.0       1.0      0.0         0.0   \n",
       "\n",
       "   is_VIRGIN ISLANDS  is_VIRGINIA  is_WASHINGTON  is_WEST VIRGINIA  \\\n",
       "0                0.0          0.0            0.0               0.0   \n",
       "1                0.0          0.0            0.0               0.0   \n",
       "2                0.0          0.0            0.0               0.0   \n",
       "3                0.0          0.0            0.0               0.0   \n",
       "4                0.0          0.0            0.0               0.0   \n",
       "\n",
       "   is_WISCONSIN  is_WYOMING  \n",
       "0           0.0         0.0  \n",
       "1           0.0         0.0  \n",
       "2           0.0         0.0  \n",
       "3           0.0         0.0  \n",
       "4           0.0         0.0  \n",
       "\n",
       "[5 rows x 68 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# tor_all.head()\n",
    "tor_all_1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['TD'] = df1['TOTAL_DEATHS'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['TI'] = df1['TOTAL_INJURIES'].apply(lambda x: 1 if x > 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    654313.000000\n",
       "mean          0.010668\n",
       "std           0.102732\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max           1.000000\n",
       "Name: TI, dtype: float64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1['TI'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# is_tornado_model['CASUALTIES'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([df1, tor_all, tor_all_1], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>INJURIES_INDIRECT</th>\n",
       "      <th>TOTAL_INJURIES</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "      <th>DEATHS_INDIRECT</th>\n",
       "      <th>TOTAL_DEATHS</th>\n",
       "      <th>CASUALTIES</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>...</th>\n",
       "      <th>is_TENNESSEE</th>\n",
       "      <th>is_TEXAS</th>\n",
       "      <th>is_UTAH</th>\n",
       "      <th>is_VERMONT</th>\n",
       "      <th>is_VIRGIN ISLANDS</th>\n",
       "      <th>is_VIRGINIA</th>\n",
       "      <th>is_WASHINGTON</th>\n",
       "      <th>is_WEST VIRGINIA</th>\n",
       "      <th>is_WISCONSIN</th>\n",
       "      <th>is_WYOMING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.543130e+05</td>\n",
       "      <td>6.543130e+05</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>6.543130e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.892418e+06</td>\n",
       "      <td>8.729241e+04</td>\n",
       "      <td>0.053577</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.010879</td>\n",
       "      <td>0.077839</td>\n",
       "      <td>3.812313e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.069227</td>\n",
       "      <td>0.006355</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.028853</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>0.010217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.418225e+06</td>\n",
       "      <td>7.913208e+04</td>\n",
       "      <td>2.914238</td>\n",
       "      <td>3.024913</td>\n",
       "      <td>4.203051</td>\n",
       "      <td>0.284431</td>\n",
       "      <td>0.062737</td>\n",
       "      <td>0.297602</td>\n",
       "      <td>4.334101</td>\n",
       "      <td>3.506985e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152823</td>\n",
       "      <td>0.253840</td>\n",
       "      <td>0.079463</td>\n",
       "      <td>0.081423</td>\n",
       "      <td>0.016445</td>\n",
       "      <td>0.167394</td>\n",
       "      <td>0.071041</td>\n",
       "      <td>0.107334</td>\n",
       "      <td>0.152007</td>\n",
       "      <td>0.100561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.419000e+03</td>\n",
       "      <td>7.650000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.808540e+05</td>\n",
       "      <td>3.058300e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.533630e+05</td>\n",
       "      <td>5.888900e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.371730e+06</td>\n",
       "      <td>1.625420e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.535308e+06</td>\n",
       "      <td>1.152422e+06</td>\n",
       "      <td>1150.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>2409.000000</td>\n",
       "      <td>1.790000e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           EVENT_ID    EPISODE_ID  INJURIES_DIRECT  INJURIES_INDIRECT  \\\n",
       "count  6.543130e+05  6.543130e+05    654313.000000      654313.000000   \n",
       "mean   1.892418e+06  8.729241e+04         0.053577           0.013384   \n",
       "std    2.418225e+06  7.913208e+04         2.914238           3.024913   \n",
       "min    3.419000e+03  7.650000e+02         0.000000           0.000000   \n",
       "25%    1.808540e+05  3.058300e+04         0.000000           0.000000   \n",
       "50%    3.533630e+05  5.888900e+04         0.000000           0.000000   \n",
       "75%    5.371730e+06  1.625420e+05         0.000000           0.000000   \n",
       "max    5.535308e+06  1.152422e+06      1150.000000        2400.000000   \n",
       "\n",
       "       TOTAL_INJURIES  DEATHS_DIRECT  DEATHS_INDIRECT   TOTAL_DEATHS  \\\n",
       "count   654313.000000  654313.000000    654313.000000  654313.000000   \n",
       "mean         0.066960       0.008861         0.002017       0.010879   \n",
       "std          4.203051       0.284431         0.062737       0.297602   \n",
       "min          0.000000       0.000000         0.000000       0.000000   \n",
       "25%          0.000000       0.000000         0.000000       0.000000   \n",
       "50%          0.000000       0.000000         0.000000       0.000000   \n",
       "75%          0.000000       0.000000         0.000000       0.000000   \n",
       "max       2400.000000     158.000000        11.000000     161.000000   \n",
       "\n",
       "          CASUALTIES  DAMAGE_PROPERTY      ...         is_TENNESSEE  \\\n",
       "count  654313.000000     6.543130e+05      ...        654313.000000   \n",
       "mean        0.077839     3.812313e+05      ...             0.023927   \n",
       "std         4.334101     3.506985e+07      ...             0.152823   \n",
       "min         0.000000     0.000000e+00      ...             0.000000   \n",
       "25%         0.000000     0.000000e+00      ...             0.000000   \n",
       "50%         0.000000     0.000000e+00      ...             0.000000   \n",
       "75%         0.000000     0.000000e+00      ...             0.000000   \n",
       "max      2409.000000     1.790000e+10      ...             1.000000   \n",
       "\n",
       "            is_TEXAS        is_UTAH     is_VERMONT  is_VIRGIN ISLANDS  \\\n",
       "count  654313.000000  654313.000000  654313.000000      654313.000000   \n",
       "mean        0.069227       0.006355       0.006674           0.000271   \n",
       "std         0.253840       0.079463       0.081423           0.016445   \n",
       "min         0.000000       0.000000       0.000000           0.000000   \n",
       "25%         0.000000       0.000000       0.000000           0.000000   \n",
       "50%         0.000000       0.000000       0.000000           0.000000   \n",
       "75%         0.000000       0.000000       0.000000           0.000000   \n",
       "max         1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "         is_VIRGINIA  is_WASHINGTON  is_WEST VIRGINIA   is_WISCONSIN  \\\n",
       "count  654313.000000  654313.000000     654313.000000  654313.000000   \n",
       "mean        0.028853       0.005072          0.011657       0.023666   \n",
       "std         0.167394       0.071041          0.107334       0.152007   \n",
       "min         0.000000       0.000000          0.000000       0.000000   \n",
       "25%         0.000000       0.000000          0.000000       0.000000   \n",
       "50%         0.000000       0.000000          0.000000       0.000000   \n",
       "75%         0.000000       0.000000          0.000000       0.000000   \n",
       "max         1.000000       1.000000          1.000000       1.000000   \n",
       "\n",
       "          is_WYOMING  \n",
       "count  654313.000000  \n",
       "mean        0.010217  \n",
       "std         0.100561  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 110 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "is_all_model = df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EVENT_ID</th>\n",
       "      <th>EPISODE_ID</th>\n",
       "      <th>INJURIES_DIRECT</th>\n",
       "      <th>INJURIES_INDIRECT</th>\n",
       "      <th>TOTAL_INJURIES</th>\n",
       "      <th>DEATHS_DIRECT</th>\n",
       "      <th>DEATHS_INDIRECT</th>\n",
       "      <th>TOTAL_DEATHS</th>\n",
       "      <th>CASUALTIES</th>\n",
       "      <th>DAMAGE_PROPERTY</th>\n",
       "      <th>...</th>\n",
       "      <th>is_TENNESSEE</th>\n",
       "      <th>is_TEXAS</th>\n",
       "      <th>is_UTAH</th>\n",
       "      <th>is_VERMONT</th>\n",
       "      <th>is_VIRGIN ISLANDS</th>\n",
       "      <th>is_VIRGINIA</th>\n",
       "      <th>is_WASHINGTON</th>\n",
       "      <th>is_WEST VIRGINIA</th>\n",
       "      <th>is_WISCONSIN</th>\n",
       "      <th>is_WYOMING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>6.543130e+05</td>\n",
       "      <td>6.543130e+05</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>6.543130e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "      <td>654313.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.892418e+06</td>\n",
       "      <td>8.729241e+04</td>\n",
       "      <td>0.053577</td>\n",
       "      <td>0.013384</td>\n",
       "      <td>0.066960</td>\n",
       "      <td>0.008861</td>\n",
       "      <td>0.002017</td>\n",
       "      <td>0.010879</td>\n",
       "      <td>0.077839</td>\n",
       "      <td>3.812313e+05</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023927</td>\n",
       "      <td>0.069227</td>\n",
       "      <td>0.006355</td>\n",
       "      <td>0.006674</td>\n",
       "      <td>0.000271</td>\n",
       "      <td>0.028853</td>\n",
       "      <td>0.005072</td>\n",
       "      <td>0.011657</td>\n",
       "      <td>0.023666</td>\n",
       "      <td>0.010217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.418225e+06</td>\n",
       "      <td>7.913208e+04</td>\n",
       "      <td>2.914238</td>\n",
       "      <td>3.024913</td>\n",
       "      <td>4.203051</td>\n",
       "      <td>0.284431</td>\n",
       "      <td>0.062737</td>\n",
       "      <td>0.297602</td>\n",
       "      <td>4.334101</td>\n",
       "      <td>3.506985e+07</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152823</td>\n",
       "      <td>0.253840</td>\n",
       "      <td>0.079463</td>\n",
       "      <td>0.081423</td>\n",
       "      <td>0.016445</td>\n",
       "      <td>0.167394</td>\n",
       "      <td>0.071041</td>\n",
       "      <td>0.107334</td>\n",
       "      <td>0.152007</td>\n",
       "      <td>0.100561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>3.419000e+03</td>\n",
       "      <td>7.650000e+02</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.808540e+05</td>\n",
       "      <td>3.058300e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.533630e+05</td>\n",
       "      <td>5.888900e+04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>5.371730e+06</td>\n",
       "      <td>1.625420e+05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.535308e+06</td>\n",
       "      <td>1.152422e+06</td>\n",
       "      <td>1150.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>2400.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>161.000000</td>\n",
       "      <td>2409.000000</td>\n",
       "      <td>1.790000e+10</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 110 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           EVENT_ID    EPISODE_ID  INJURIES_DIRECT  INJURIES_INDIRECT  \\\n",
       "count  6.543130e+05  6.543130e+05    654313.000000      654313.000000   \n",
       "mean   1.892418e+06  8.729241e+04         0.053577           0.013384   \n",
       "std    2.418225e+06  7.913208e+04         2.914238           3.024913   \n",
       "min    3.419000e+03  7.650000e+02         0.000000           0.000000   \n",
       "25%    1.808540e+05  3.058300e+04         0.000000           0.000000   \n",
       "50%    3.533630e+05  5.888900e+04         0.000000           0.000000   \n",
       "75%    5.371730e+06  1.625420e+05         0.000000           0.000000   \n",
       "max    5.535308e+06  1.152422e+06      1150.000000        2400.000000   \n",
       "\n",
       "       TOTAL_INJURIES  DEATHS_DIRECT  DEATHS_INDIRECT   TOTAL_DEATHS  \\\n",
       "count   654313.000000  654313.000000    654313.000000  654313.000000   \n",
       "mean         0.066960       0.008861         0.002017       0.010879   \n",
       "std          4.203051       0.284431         0.062737       0.297602   \n",
       "min          0.000000       0.000000         0.000000       0.000000   \n",
       "25%          0.000000       0.000000         0.000000       0.000000   \n",
       "50%          0.000000       0.000000         0.000000       0.000000   \n",
       "75%          0.000000       0.000000         0.000000       0.000000   \n",
       "max       2400.000000     158.000000        11.000000     161.000000   \n",
       "\n",
       "          CASUALTIES  DAMAGE_PROPERTY      ...         is_TENNESSEE  \\\n",
       "count  654313.000000     6.543130e+05      ...        654313.000000   \n",
       "mean        0.077839     3.812313e+05      ...             0.023927   \n",
       "std         4.334101     3.506985e+07      ...             0.152823   \n",
       "min         0.000000     0.000000e+00      ...             0.000000   \n",
       "25%         0.000000     0.000000e+00      ...             0.000000   \n",
       "50%         0.000000     0.000000e+00      ...             0.000000   \n",
       "75%         0.000000     0.000000e+00      ...             0.000000   \n",
       "max      2409.000000     1.790000e+10      ...             1.000000   \n",
       "\n",
       "            is_TEXAS        is_UTAH     is_VERMONT  is_VIRGIN ISLANDS  \\\n",
       "count  654313.000000  654313.000000  654313.000000      654313.000000   \n",
       "mean        0.069227       0.006355       0.006674           0.000271   \n",
       "std         0.253840       0.079463       0.081423           0.016445   \n",
       "min         0.000000       0.000000       0.000000           0.000000   \n",
       "25%         0.000000       0.000000       0.000000           0.000000   \n",
       "50%         0.000000       0.000000       0.000000           0.000000   \n",
       "75%         0.000000       0.000000       0.000000           0.000000   \n",
       "max         1.000000       1.000000       1.000000           1.000000   \n",
       "\n",
       "         is_VIRGINIA  is_WASHINGTON  is_WEST VIRGINIA   is_WISCONSIN  \\\n",
       "count  654313.000000  654313.000000     654313.000000  654313.000000   \n",
       "mean        0.028853       0.005072          0.011657       0.023666   \n",
       "std         0.167394       0.071041          0.107334       0.152007   \n",
       "min         0.000000       0.000000          0.000000       0.000000   \n",
       "25%         0.000000       0.000000          0.000000       0.000000   \n",
       "50%         0.000000       0.000000          0.000000       0.000000   \n",
       "75%         0.000000       0.000000          0.000000       0.000000   \n",
       "max         1.000000       1.000000          1.000000       1.000000   \n",
       "\n",
       "          is_WYOMING  \n",
       "count  654313.000000  \n",
       "mean        0.010217  \n",
       "std         0.100561  \n",
       "min         0.000000  \n",
       "25%         0.000000  \n",
       "50%         0.000000  \n",
       "75%         0.000000  \n",
       "max         1.000000  \n",
       "\n",
       "[8 rows x 110 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_all_model.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "is_all_model.drop([ 'YEAR', u'TOTAL_INJURIES', u'DAMAGE_PROPERTY', u'DAMAGE_CROPS',\n",
    "       u'COMBINED_DAMAGE', 'EVENT_ID', u'EPISODE_ID',u'INJURIES_DIRECT', u'CASUALTIES', u'INJURIES_INDIRECT','EVENT_TYPE', 'DATE', u'CZ_NAME', u'CATEGORY', u'STATE',\n",
    "       u'MONTH_NAME', u'TOR_F_SCALE', u'EPISODE_NARRATIVE', u'EVENT_NARRATIVE',u'DEATHS_DIRECT',\n",
    "       u'DEATHS_INDIRECT', u'TOTAL_DEATHS'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'TOR_LENGTH', u'TOR_WIDTH', u'is_Drought', u'is_Flood',\n",
       "       u'is_Winter Storm', u'is_OTHER', u'is_High Wind', u'is_ThunderStorm',\n",
       "       u'is_Wildfire', u'is_Rip Current', u'is_Tropical Storm', u'is_Tornado',\n",
       "       u'is_Hurricane', u'is_Tsunami', u'is_Landslide', u'TD', u'TI',\n",
       "       u'is_EF0', u'is_EF1', u'is_EF2', u'is_EF3', u'is_EF4', u'is_EF5',\n",
       "       u'is_F0', u'is_F1', u'is_F2', u'is_F3', u'is_F4', u'is_ALABAMA',\n",
       "       u'is_ALASKA', u'is_AMERICAN SAMOA', u'is_ARIZONA', u'is_ARKANSAS',\n",
       "       u'is_ATLANTIC NORTH', u'is_ATLANTIC SOUTH', u'is_CALIFORNIA',\n",
       "       u'is_COLORADO', u'is_CONNECTICUT', u'is_DELAWARE',\n",
       "       u'is_DISTRICT OF COLUMBIA', u'is_E PACIFIC', u'is_FLORIDA',\n",
       "       u'is_GEORGIA', u'is_GUAM', u'is_GULF OF ALASKA', u'is_GULF OF MEXICO',\n",
       "       u'is_HAWAII', u'is_HAWAII WATERS', u'is_IDAHO', u'is_ILLINOIS',\n",
       "       u'is_INDIANA', u'is_IOWA', u'is_KANSAS', u'is_KENTUCKY',\n",
       "       u'is_LAKE ERIE', u'is_LAKE HURON', u'is_LAKE MICHIGAN',\n",
       "       u'is_LAKE ONTARIO', u'is_LAKE ST CLAIR', u'is_LAKE SUPERIOR',\n",
       "       u'is_LOUISIANA', u'is_MAINE', u'is_MARYLAND', u'is_MASSACHUSETTS',\n",
       "       u'is_MICHIGAN', u'is_MINNESOTA', u'is_MISSISSIPPI', u'is_MISSOURI',\n",
       "       u'is_MONTANA', u'is_NEBRASKA', u'is_NEVADA', u'is_NEW HAMPSHIRE',\n",
       "       u'is_NEW JERSEY', u'is_NEW MEXICO', u'is_NEW YORK',\n",
       "       u'is_NORTH CAROLINA', u'is_NORTH DAKOTA', u'is_OHIO', u'is_OKLAHOMA',\n",
       "       u'is_OREGON', u'is_PENNSYLVANIA', u'is_PUERTO RICO', u'is_RHODE ISLAND',\n",
       "       u'is_SOUTH CAROLINA', u'is_SOUTH DAKOTA', u'is_ST LAWRENCE R',\n",
       "       u'is_TENNESSEE', u'is_TEXAS', u'is_UTAH', u'is_VERMONT',\n",
       "       u'is_VIRGIN ISLANDS', u'is_VIRGINIA', u'is_WASHINGTON',\n",
       "       u'is_WEST VIRGINIA', u'is_WISCONSIN', u'is_WYOMING'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_all_model.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4481"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_all_model.TD.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier()\n",
    "\n",
    "y1 = LabelEncoder().fit_transform(is_all_model['TD'])\n",
    "x1 = is_all_model.drop(['TI','TD'], axis=1)\n",
    "\n",
    "model1 = rf_model.fit(x1,y1)\n",
    "# model1.score(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[649693,    139],\n",
       "       [  3781,    700]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_class1 = model1.predict(x1)\n",
    "confusion_matrix(y1, predicted_class1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00    649832\n",
      "          1       0.83      0.16      0.26      4481\n",
      "\n",
      "avg / total       0.99      0.99      0.99    654313\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print classification_report(y1, predicted_class1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision\n",
    "\n",
    "    Precision is the number of True Positives divided by the number of True Positives and False Positives. Put another way, it is the number of positive predictions divided by the total number of positive class values predicted. It is also called the Positive Predictive Value (PPV).\n",
    "\n",
    "    Precision can be thought of as a measure of a classifiers exactness. A low precision can also indicate a large number of False Positives.\n",
    "\n",
    "\n",
    "\n",
    "## Recall\n",
    "\n",
    "    Recall is the number of True Positives divided by the number of True Positives and the number of False Negatives. Put another way it is the number of positive predictions divided by the number of positive class values in the test data. It is also called Sensitivity or the True Positive Rate.\n",
    "\n",
    "    Recall can be thought of as a measure of a classifiers completeness. A low recall indicates many False Negatives.\n",
    "\n",
    "\n",
    "    \n",
    "    F1 Score\n",
    "\n",
    "    The F1 Score is the 2*((precision*recall)/(precision+recall)). It is also called the F Score or the F Measure. Put another way, the F1 score conveys the balance between the precision and the recall.\n",
    "\n",
    "\n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(11,8))\n",
    "# sbn.heatmap(is_tornado_model.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(x1,y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((490734, 94), (163579, 94), (490734,), (163579,))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfTTS = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99405788064409639"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTTS = rfTTS.fit(x_train, y_train)\n",
    "\n",
    "modelTTS.score(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[487277,     99],\n",
       "       [  2817,    541]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_train = modelTTS.predict(x_train)\n",
    "confusion_matrix(y_train, predict_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.99347715782588231"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelTTS.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[162398,     58],\n",
       "       [  1009,    114]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_test = modelTTS.predict(x_test)\n",
    "confusion_matrix(y_test, predict_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Y_score = rfTTS.decision_function(x_test)\n",
    "\n",
    "# FPR = dict()\n",
    "# TPR = dict()\n",
    "# ROC_AUC = dict()\n",
    "\n",
    "# print roc_curve(y_test, Y_score)\n",
    "\n",
    "# FPR[1], TPR[1], _ = roc_curve(y_test, Y_score)\n",
    "# ROC_AUC[1] = auc(FPR[1], TPR[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plt.figure(figsize=[11,9])\n",
    "# plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4)\n",
    "# plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "# plt.xlim([0.0, 1.0])\n",
    "# plt.ylim([0.0, 1.05])\n",
    "# plt.xlabel('False Positive Rate', fontsize=18)\n",
    "# plt.ylabel('True Positive Rate', fontsize=18)\n",
    "# plt.title('Tornado Deaths Predictor', fontsize=18)\n",
    "# plt.legend(loc=\"lower right\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# #GRidsearch\n",
    "# rf_parameters = {\n",
    "#     \"n_estimators\" : [10, 25, 50, 100, 500],\n",
    "#     \"max_depth\" : [2,3,5,7,10, None],\n",
    "#     \"max_features\" : [0.25, 0.5, 0.75, 1.0]\n",
    "# }\n",
    "# modelgs = RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search = GridSearchCV(estimator=modelgs,\n",
    "#                           param_grid=rf_parameters\n",
    "#                            , n_jobs=-1) \n",
    "# grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ygs = LabelEncoder().fit_transform(y_test)\n",
    "# xgs = x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search.fit(xgs, ygs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_.fit(x1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grid_search.best_estimator_.score(x1, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Confusion matrix for final grid-search model\n",
    "\n",
    "# gspredict = grid_search.predict(xgs)\n",
    "# confusion_matrix(ygs, gspredict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df4 = is_all_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert dataframe to csv\n",
    "\n",
    "df4.to_csv('df4.csv')\n",
    "#This worked!  Thank God this was successful on Tuesday school!\n",
    "# 10 years worth of data (654313, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = is_all_model.sample(frac=0.1)\n",
    "df.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOR_LENGTH</th>\n",
       "      <th>TOR_WIDTH</th>\n",
       "      <th>is_Drought</th>\n",
       "      <th>is_Flood</th>\n",
       "      <th>is_Winter Storm</th>\n",
       "      <th>is_OTHER</th>\n",
       "      <th>is_High Wind</th>\n",
       "      <th>is_ThunderStorm</th>\n",
       "      <th>is_Wildfire</th>\n",
       "      <th>is_Rip Current</th>\n",
       "      <th>...</th>\n",
       "      <th>is_TENNESSEE</th>\n",
       "      <th>is_TEXAS</th>\n",
       "      <th>is_UTAH</th>\n",
       "      <th>is_VERMONT</th>\n",
       "      <th>is_VIRGIN ISLANDS</th>\n",
       "      <th>is_VIRGINIA</th>\n",
       "      <th>is_WASHINGTON</th>\n",
       "      <th>is_WEST VIRGINIA</th>\n",
       "      <th>is_WISCONSIN</th>\n",
       "      <th>is_WYOMING</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "      <td>65431.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.069032</td>\n",
       "      <td>3.762667</td>\n",
       "      <td>0.068286</td>\n",
       "      <td>0.104431</td>\n",
       "      <td>0.412526</td>\n",
       "      <td>0.010163</td>\n",
       "      <td>0.043756</td>\n",
       "      <td>0.311045</td>\n",
       "      <td>0.006067</td>\n",
       "      <td>0.001009</td>\n",
       "      <td>...</td>\n",
       "      <td>0.023674</td>\n",
       "      <td>0.069982</td>\n",
       "      <td>0.005991</td>\n",
       "      <td>0.006358</td>\n",
       "      <td>0.000352</td>\n",
       "      <td>0.029344</td>\n",
       "      <td>0.005319</td>\n",
       "      <td>0.012379</td>\n",
       "      <td>0.023659</td>\n",
       "      <td>0.010729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.875917</td>\n",
       "      <td>49.205042</td>\n",
       "      <td>0.252237</td>\n",
       "      <td>0.305821</td>\n",
       "      <td>0.492293</td>\n",
       "      <td>0.100301</td>\n",
       "      <td>0.204553</td>\n",
       "      <td>0.462925</td>\n",
       "      <td>0.077658</td>\n",
       "      <td>0.031744</td>\n",
       "      <td>...</td>\n",
       "      <td>0.152032</td>\n",
       "      <td>0.255119</td>\n",
       "      <td>0.077170</td>\n",
       "      <td>0.079483</td>\n",
       "      <td>0.018746</td>\n",
       "      <td>0.168770</td>\n",
       "      <td>0.072735</td>\n",
       "      <td>0.110573</td>\n",
       "      <td>0.151984</td>\n",
       "      <td>0.103024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "      <td>2815.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 85 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         TOR_LENGTH     TOR_WIDTH    is_Drought      is_Flood  \\\n",
       "count  65431.000000  65431.000000  65431.000000  65431.000000   \n",
       "mean       0.069032      3.762667      0.068286      0.104431   \n",
       "std        0.875917     49.205042      0.252237      0.305821   \n",
       "min        0.000000      0.000000      0.000000      0.000000   \n",
       "25%        0.000000      0.000000      0.000000      0.000000   \n",
       "50%        0.000000      0.000000      0.000000      0.000000   \n",
       "75%        0.000000      0.000000      0.000000      0.000000   \n",
       "max      100.000000   2815.000000      1.000000      1.000000   \n",
       "\n",
       "       is_Winter Storm      is_OTHER  is_High Wind  is_ThunderStorm  \\\n",
       "count     65431.000000  65431.000000  65431.000000     65431.000000   \n",
       "mean          0.412526      0.010163      0.043756         0.311045   \n",
       "std           0.492293      0.100301      0.204553         0.462925   \n",
       "min           0.000000      0.000000      0.000000         0.000000   \n",
       "25%           0.000000      0.000000      0.000000         0.000000   \n",
       "50%           0.000000      0.000000      0.000000         0.000000   \n",
       "75%           1.000000      0.000000      0.000000         1.000000   \n",
       "max           1.000000      1.000000      1.000000         1.000000   \n",
       "\n",
       "        is_Wildfire  is_Rip Current      ...       is_TENNESSEE      is_TEXAS  \\\n",
       "count  65431.000000    65431.000000      ...       65431.000000  65431.000000   \n",
       "mean       0.006067        0.001009      ...           0.023674      0.069982   \n",
       "std        0.077658        0.031744      ...           0.152032      0.255119   \n",
       "min        0.000000        0.000000      ...           0.000000      0.000000   \n",
       "25%        0.000000        0.000000      ...           0.000000      0.000000   \n",
       "50%        0.000000        0.000000      ...           0.000000      0.000000   \n",
       "75%        0.000000        0.000000      ...           0.000000      0.000000   \n",
       "max        1.000000        1.000000      ...           1.000000      1.000000   \n",
       "\n",
       "            is_UTAH    is_VERMONT  is_VIRGIN ISLANDS   is_VIRGINIA  \\\n",
       "count  65431.000000  65431.000000       65431.000000  65431.000000   \n",
       "mean       0.005991      0.006358           0.000352      0.029344   \n",
       "std        0.077170      0.079483           0.018746      0.168770   \n",
       "min        0.000000      0.000000           0.000000      0.000000   \n",
       "25%        0.000000      0.000000           0.000000      0.000000   \n",
       "50%        0.000000      0.000000           0.000000      0.000000   \n",
       "75%        0.000000      0.000000           0.000000      0.000000   \n",
       "max        1.000000      1.000000           1.000000      1.000000   \n",
       "\n",
       "       is_WASHINGTON  is_WEST VIRGINIA  is_WISCONSIN    is_WYOMING  \n",
       "count   65431.000000      65431.000000  65431.000000  65431.000000  \n",
       "mean        0.005319          0.012379      0.023659      0.010729  \n",
       "std         0.072735          0.110573      0.151984      0.103024  \n",
       "min         0.000000          0.000000      0.000000      0.000000  \n",
       "25%         0.000000          0.000000      0.000000      0.000000  \n",
       "50%         0.000000          0.000000      0.000000      0.000000  \n",
       "75%         0.000000          0.000000      0.000000      0.000000  \n",
       "max         1.000000          1.000000      1.000000      1.000000  \n",
       "\n",
       "[8 rows x 85 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y1 = LabelEncoder().fit_transform(df['TD'])\n",
    "x1 = df.drop(['TD'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45801, 84) (45801,)\n",
      "(19630, 84) (19630,)\n"
     ]
    }
   ],
   "source": [
    "# setting up train and test data for X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(x1, y1, test_size=0.3)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_models = {\n",
    "   'knn' : KNeighborsClassifier(),\n",
    "   'knn_bag' : BaggingClassifier(KNeighborsClassifier()),\n",
    "   'logreg' : LogisticRegression(),\n",
    "   'logreg_bag' : BaggingClassifier(LogisticRegression()),\n",
    "   'tree' : DecisionTreeClassifier(),\n",
    "   'tree_bag' : BaggingClassifier(DecisionTreeClassifier()),\n",
    "   'forest' : RandomForestClassifier(),\n",
    "   'extra_tree' : ExtraTreesClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "def evaluate_model(X_train, X_test, \n",
    "                   y_train, y_test,\n",
    "                  model):\n",
    "    pick_a_model = global_models[model]\n",
    "    pick_a_model.fit(X_train, y_train)\n",
    "    print 'This is the model:', (model)\n",
    "    print 'Train model score:', pick_a_model.score(X_train, y_train)\n",
    "    predictions = pick_a_model.predict(X_test)\n",
    "    print '\\nConfusion matrix: \\n\\n', confusion_matrix(y_test, predictions)\n",
    "    print '\\nClassification report: \\n\\n', classification_report(y_test, predictions)\n",
    "    results.update({model: pick_a_model.score(X_test, y_test)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the model: knn\n",
      "Train model score: 0.993995764285\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19503     2]\n",
      " [  111    14]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.88      0.11      0.20       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: logreg_bag\n",
      "Train model score: 0.993362590336\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19500     5]\n",
      " [  117     8]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.62      0.06      0.12       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: knn_bag\n",
      "Train model score: 0.994017597869\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19503     2]\n",
      " [  111    14]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.88      0.11      0.20       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: extra_tree_bag\n",
      "Train model score: 0.994803606908\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19496     9]\n",
      " [  105    20]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.69      0.16      0.26       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: forest\n",
      "Train model score: 0.994738106155\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19493    12]\n",
      " [  104    21]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.64      0.17      0.27       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: tree_bag\n",
      "Train model score: 0.994803606908\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19491    14]\n",
      " [  103    22]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.61      0.18      0.27       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: forest_bag\n",
      "Train model score: 0.994410602389\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19497     8]\n",
      " [  106    19]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.70      0.15      0.25       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: extra_tree\n",
      "Train model score: 0.994956441999\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19492    13]\n",
      " [  104    21]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.62      0.17      0.26       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: tree\n",
      "Train model score: 0.994956441999\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19486    19]\n",
      " [  103    22]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.54      0.18      0.27       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n",
      "This is the model: logreg\n",
      "Train model score: 0.993362590336\n",
      "\n",
      "Confusion matrix: \n",
      "\n",
      "[[19500     5]\n",
      " [  118     7]]\n",
      "\n",
      "Classification report: \n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       0.99      1.00      1.00     19505\n",
      "          1       0.58      0.06      0.10       125\n",
      "\n",
      "avg / total       0.99      0.99      0.99     19630\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for key in global_models:\n",
    "    evaluate_model(X_train, X_test, y_train, y_test, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-26e279b9ead7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#convert dataframe to csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'df4.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m#This worked!  Thank God this was successful on Tuesday school!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# 10 years worth of data (654313, 24)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "#convert dataframe to csv\n",
    "\n",
    "df.to_csv('df4.csv')\n",
    "#This worked!  Thank God this was successful on Tuesday school!\n",
    "# 10 years worth of data (654313, 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## plots ROC curve\n",
    "def plot_roc(x_test, y_test, model):\n",
    "    fpr, tpr, _ = roc_curve(y_test, model.predict_proba(x_test)[:,1])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    print 'ROC AUC: %0.2f' % roc_auc\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for model in global_models:\n",
    "    plot_roc(X_test, y_test, model)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
