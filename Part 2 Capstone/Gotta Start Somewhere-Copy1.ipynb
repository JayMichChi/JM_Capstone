{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JayMichChi Part 2 Capstone\n",
    "### Goal: Jays summary notebook outlining my project's goals, methods, models, and EDA.\n",
    "\n",
    "This will be an overview of my approach with a well-articulated summary that includes your problem statement, outlines your proposed methods and models, defines any risks & assumptions, and includes any revisions from your initial goals & criteria, as needed.\n",
    "\n",
    "In addition, I will explain how I intend to store my data. \n",
    "\n",
    "You'll want to describe and create a data dictionary for your dataset(s), and perform your initial exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My hypothesis will attempt to:\n",
    "Reasonably predict the amount of annual deaths and property damage due to certain types of storm occurrences based on state and storm type past data.\n",
    "\n",
    "### Capstone Project\n",
    "My capstone will be gathering data from the National Centers for Environmental Information, particularly information from 2003 to 2013, and will look at unusual storm occurrences and unusual weather phenomena, and mores specifically, information on storm paths, deaths, injuries, and property damage.\n",
    "\n",
    "### Outline of Proposed Data Collection Methods \n",
    "National Centers for Environmental Information (NCEI) is responsible for hosting and providing access to one of the most significant archives on Earth, with comprehensive oceanic, atmospheric, and geophysical data. From the depths of the ocean to the surface of the sun and from million-year-old sediment records to near real-time satellite images, NCEI is the Nation’s leading authority for environmental information.  It is from this source that I will primarily ingest data on the following: https://www.ncdc.noaa.gov/stormevents/\n",
    "\n",
    "### Storm Events Database\n",
    "The Storm Events Database contains a chronological listing, by states, of storm occurrences and unusual weather phenomena. Reports contain information on storm paths, deaths, injuries, and property damage.s\n",
    "\n",
    "The Storm Events Database currently contains data from January 1950 to August 2016, as entered by NOAA's National Weather Service (NWS). Due to changes in the data collection and processing procedures over time, there are unique periods of record available depending on the event type. I have chosen to look at the data from 1996 to 2013.\n",
    "\n",
    "### Data base\n",
    "I was able to find the 10 years of data on the following NCDC website FTP server. \n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/\n",
    "\n",
    "    Index of /pub/data/swdi/stormevents/csvfiles/legacy/\n",
    "    Name\tSize\tDate Modified\n",
    "    ugc_areas.csv\t902 kB\t2/18/14, 12:00:00 AM\n",
    "    stormdata_2013.csv\t53.1 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2012.csv\t64.3 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2011.csv\t82.3 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2010.csv\t62.5 MB\t1/31/14, 12:00:00 AM\n",
    "    Stormdata_2009.csv\t49.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2008.csv\t60.6 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2007.csv\t49.7 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2006.csv\t26.0 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2005.csv\t34.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2004.csv\t33.1 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2003.csv\t32.1 MB\t9/19/13, 12:00:00 AM\n",
    "  \n",
    "I have successfully downloaded all 18 CSV files. My work will show that i merged the data via pandas concant processes. Additionally, I will have to clean the data, reformat dates, categorize certain feature series, and perhaps use dummy variables to allow me to model the data successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Proposed Data Cleaning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks & Assumptions\n",
    "\n",
    "There are some risks and assumptions that I need to incorporate into my capstone and the data. \n",
    "\n",
    "First, I will be looking at the dollar damage. Certainly inflation will come into play as the cost of crops and property replacement costs fluctuate. This is one of the primary reasons that I am reducing my data to just 10 years. \n",
    "\n",
    "(The other reason is the sheer size of the database) \n",
    "\n",
    "There are also confounding factors that will effect the results: For instance, absolutely/categorically attributing  fatalities to a weather event may not be completely correct, and may in fact occcur our of coincidence. Additionally, there are extenuating factors that could effect the cost or fatality number. For instance, the responsiveness of the National Guard, or massive federal funding may effect the amount of fatalities, or the extent of damage. \n",
    "\n",
    "In short, the assumption we make is that all these extenuating factors will not adversely effect the results because our dataset of over 600,000 instances is so large. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Proposed Data Modelling\n",
    "\n",
    "I expect that once i complete cleaning the data set, I will then fitting this data to a few modeles.\n",
    "\n",
    "My initial inclination is to see if I can find any coorelations in the 58 columns of data. \n",
    "\n",
    "I believe that Logistic Regression model and Linear Regression models will be the first two models I will run. \n",
    "\n",
    "\n",
    "I will perform a train test split on just a segment of random data I grab from my database. This will allow me to work with dataset that is more managable for my laptop. \n",
    "\n",
    "Once cleaned, tts, fit and run, I will attempt to graph the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local database\n",
    "Below I grab my data from NOAA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here is my scroll. it works, but the database url_base changes in y2010, \n",
    "# so i will re-run this with the updated base_url\n",
    "import pandas as pd\n",
    "import time\n",
    "url_list = ['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2003.csv']\n",
    "web_count= 2003\n",
    "for pages in range(1,7):\n",
    "    web_count= 1 + web_count\n",
    "    count_update=str(web_count)\n",
    "    base_url = 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_' + count_update + '.csv'\n",
    "    url_list.append(base_url)\n",
    "url_list\n",
    "# This works wonderfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is my one time function that will load, append\n",
    "# for url in url_list:\n",
    "data1 = []\n",
    "for url in url_list:\n",
    "    page = pd.read_csv((url)) \n",
    "    data1.append(page)\n",
    "    time.sleep(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_all = pd.concat(data1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data1_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#as above\n",
    "# so i will re-run this with the updated base_url\n",
    "\n",
    "import time\n",
    "url_list_2 = ['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2010.csv']\n",
    "web_count= 2010\n",
    "for pages in range(1,4):\n",
    "    web_count= 1 + web_count\n",
    "    count_update=str(web_count)\n",
    "    base_url_2 = 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_' + count_update + '.csv'\n",
    "    url_list_2.append(base_url_2)\n",
    "url_list_2\n",
    "# This works wonderfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is my one time function that will load, append\n",
    "# for url in url_list:\n",
    "import pandas as pd\n",
    "data2 = []\n",
    "for url in url_list_2:\n",
    "    page = pd.read_csv((url)) \n",
    "    data2.append(page)\n",
    "    time.sleep(30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2_all=pd.concat(data2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([data1_all, data2_all], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert dataframe to csv\n",
    "\n",
    "df_all.to_csv('jm_cap.csv')\n",
    "#This worked!  Thank God this was successful on Tuesday school!\n",
    "# 10 years worth of data (654313, 58)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([u'Unnamed: 0', u'BEGIN_YEARMONTH', u'BEGIN_DAY', u'BEGIN_TIME',\n",
       "       u'END_YEARMONTH', u'END_DAY', u'END_TIME', u'EPISODE_ID', u'EVENT_ID',\n",
       "       u'STATE', u'STATE_FIPS', u'YEAR', u'MONTH_NAME', u'EVENT_TYPE',\n",
       "       u'CZ_TYPE', u'CZ_FIPS', u'CZ_NAME', u'WFO', u'BEGIN_DATE_TIME',\n",
       "       u'CZ_TIMEZONE', u'END_DATE_TIME', u'INJURIES_DIRECT',\n",
       "       u'INJURIES_INDIRECT', u'DEATHS_DIRECT', u'DEATHS_INDIRECT',\n",
       "       u'DAMAGE_PROPERTY', u'DAMAGE_CROPS', u'SOURCE', u'MAGNITUDE',\n",
       "       u'MAGNITUDE_TYPE', u'FLOOD_CAUSE', u'CATEGORY', u'TOR_F_SCALE',\n",
       "       u'TOR_LENGTH', u'TOR_WIDTH', u'TOR_OTHER_WFO', u'TOR_OTHER_CZ_STATE',\n",
       "       u'TOR_OTHER_CZ_FIPS', u'TOR_OTHER_CZ_NAME', u'BEGIN_RANGE',\n",
       "       u'BEGIN_AZIMUTH', u'BEGIN_LOCATION', u'END_RANGE', u'END_AZIMUTH',\n",
       "       u'END_LOCATION', u'BEGIN_LAT', u'BEGIN_LON', u'END_LAT', u'END_LON',\n",
       "       u'EPISODE_NARRATIVE', u'EVENT_NARRATIVE', u'LAST_MOD_DATE',\n",
       "       u'LAST_MOD_TIME', u'LAST_CERT_DATE', u'LAST_CERT_TIME', u'LAST_MOD',\n",
       "       u'LAST_CERT', u'ADDCORR_FLG', u'ADDCORR_DATE'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('jm_cap.csv',error_bad_lines=False)\n",
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DONE DATABASE IS SUCCESSFULLY DOWNLOADED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data dictionary\n",
    "\n",
    "    Storm Data Export Format, Field names\n",
    "\n",
    "    Event Details File (named event_details_YYYYMM.csv):\n",
    "    last_date_modified  Ex:  4/4/2012  11:05:44 PM, 5/3/2012  5:10:49 AM\n",
    "    MM/DD/YYYY  12 hour time AM/PM\n",
    "\n",
    "    The last date of modification by NWS.  Any corrections to the storm event/episode in question are made solely by NWS and the person that actually entered the event/episode.\n",
    "\n",
    "    last_date_certified  Ex:  5/10/2012  9:10:51 AM, 5/18/2012  12:21:15 AM \n",
    "    MM/DD/YYYY  12 hour time AM/PM\n",
    "\n",
    "    The last date of certification by NWS.  Any corrections to the storm event/episode in question are made solely by NWS and the person that actually entered the event/episode.\n",
    "\n",
    "    episode_id   Ex: 61280, 62777, 63250\n",
    "    (ID assigned by NWS to denote the storm episode; links the event details file with the information within location file)\n",
    "\n",
    "    The occurrence of storms and other significant weather phenomena having sufficient intensity to cause loss of life, injuries, significant property damage, and/or disruption to commerce.  Rare, unusual, weather phenomena that generate media attention, such as snow flurries in South Florida or the San Diego coastal area; and Other significant meteorological events, such as record maximum or minimum temperatures or precipitation that occur in connection with another event.\n",
    "\n",
    "    event_id  Ex: 383097, 374427, 364175\n",
    "\n",
    "    (Primary database key field)\n",
    "    (ID assigned by NWS to note a single, small part that goes into a specific storm episode; links the storm episode between the three files downloaded from SPC’s website)\n",
    "\n",
    "    state  Ex:  GEORGIA,  WYOMING, COLORADO\n",
    "    The state name where the event occurred (no State ID’s are included here; State Name is spelled out in ALL CAPS)\n",
    "\n",
    "    state_fips  Ex: 45, 30, 12\n",
    "    A unique number (State Federal Information Processing Standard) is assigned to the county by the National Institute for Standards and Technology (NIST).\n",
    "\n",
    "    year  Ex: 2000, 2006, 2012\n",
    "    Four digit year for the event in this record\n",
    "\n",
    "\n",
    "\n",
    "    month_name  Ex: January, February, March\n",
    "    Name of the month for the event in this record (spelled out; not abbreviated)\n",
    "\n",
    "    event_type  Ex: Hail, Thunderstorm Wind, Snow, Ice (spelled out; not abbreviated)\n",
    "\n",
    "    The only events permitted in Storm Data are listed in Table 1 of Section 2.1.1 of NWS Directive 10-1605 at http://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf. The chosen event name should be the one that most accurately describes the meteorological event leading to fatalities, injuries, damage, etc. However, significant events, such as tornadoes, having no impact or causing no damage, should also be included in Storm Data.\n",
    "\n",
    "    From Section 2.1.1 of NWS Directive 10-1605:\n",
    "    Event Name / Designator (County or Zone and whether the event happened in a (C) county/parish,(Z)zone or(M) marine\n",
    "    Astronomical Low Tide\tZ \n",
    "    Avalanche    Z \n",
    "    Blizzard    Z \n",
    "    Coastal Flood   Z \n",
    "    Cold/Wind Chill   Z \n",
    "    Debris Flow    C \n",
    "    Dense Fog    Z \n",
    "    Dense Smoke   Z \n",
    "    Drought    Z \n",
    "    Dust Devil    C \n",
    "    Dust Storm    Z \n",
    "    Excessive Heat   Z \n",
    "    Extreme Cold/Wind Chill   Z \n",
    "    Flash Flood    C   \n",
    "    Flood     C \n",
    "    Frost/Freeze    Z \n",
    "    Funnel Cloud   C \n",
    "    Freezing Fog   Z \n",
    "    Hail     C \n",
    "    Heat      Z \n",
    "    Heavy Rain    C \n",
    "    Heavy Snow    Z \n",
    "    High Surf    Z \n",
    "    High Wind    Z \n",
    "    Hurricane (Typhoon)   Z \n",
    "    Ice Storm    Z \n",
    "    Lake-Effect Snow   Z \n",
    "    Lakeshore Flood   Z \n",
    "    Lightning    C \n",
    "    Marine Hail    M \n",
    "    Marine High Wind   M \n",
    "    Marine Strong Wind   M \n",
    "    Marine Thunderstorm Wind   M  \n",
    "    Rip Current    Z \n",
    "    Seiche     Z \n",
    "    Sleet      Z \n",
    "    Storm Surge/Tide   Z \n",
    "    Strong Wind    Z \n",
    "    Thunderstorm Wind   C \n",
    "    Tornado    C \n",
    "    Tropical Depression   Z \n",
    "    Tropical Storm   Z \n",
    "    Tsunami    Z \n",
    "    Volcanic Ash   Z \n",
    "    Waterspout    M \n",
    "    Wildfire    Z \n",
    "    Winter Storm   Z \n",
    "    Winter Weather   Z\n",
    "    cz_type  Ex: C, Z , M\n",
    "\n",
    "\n",
    "    cz_fips Ex: 245, 003, 155\n",
    "    The county FIPS number is a unique number assigned to the county by the National Institute for Standards and Technology (NIST) or NWS Forecast Zone Number (See addendum)\n",
    "\n",
    "    cz_name  Ex: AIKEN, RICHMOND, BAXTER (County/Parish, Zone or Marine Name assigned to the county FIPS number or NWS Forecast Zone)\n",
    "\n",
    "    wfo  Ex: CAE, BYZ, GJT (National Weather Service Forecast Office’s area of responsibility (County Warning Area) in which the event occurred)\n",
    "\n",
    "    begin_date_time  Ex: 4/1/2012  20:48\n",
    "    MM/DD/YYYY  24 hour time AM/PM\n",
    "\n",
    "    cz_timezone  Ex: EST-5, MST-7, CST-6\n",
    "    (Time Zone for the County/Parish, Zone or Marine Name)\n",
    "    Eastern Standard Time (EST), Central Standard Time (CST), Mountain Standard Time (MST), etc.\n",
    "\n",
    "    end_date_time  Ex: 4/1/2012  21:03\n",
    "    MM/DD/YYYY  24 hour time AM/PM\n",
    "\n",
    "    injuries_direct  Ex:  1, 0, 56\n",
    "    The number of injuries directly related to the weather event\n",
    "\n",
    "    injuries_indirect  Ex:  0, 15, 87\n",
    "    The number of injuries indirectly related to the weather event\n",
    "\n",
    "    deaths_direct  Ex: 0, 45, 23\n",
    "    The number of deaths directly related to the weather event.\n",
    "\n",
    "    deaths_indirect Ex: 0, 4, 6\n",
    "    The number of deaths indirectly related to the weather event\n",
    "\n",
    "    damage_property  Ex: 10.00K, 0.00K, 10.00M\n",
    "    The estimated amount of damage to property incurred by the weather event.  (e.g. 10.00K = $10,000; 10.00M = $10,000,000)\n",
    "\n",
    "    damage_crops  Ex: 0.00K, 500.00K, 15.00M\n",
    "    The estimated amount of damage to crops incurred by the weather event   (e.g. 10.00K = $10,000; 10.00M = $10,000,000)\n",
    "    source  Ex: Public, Newspaper, Law Enforcement, Broadcast Media, ASOS, Park and Forest Service, Trained Spotter, CoCoRaHS,  etc.  (can be any entry; isn’t restricted in what’s allowed)\n",
    "    Source reporting the weather event\n",
    "\n",
    "    magnitude  Ex: 0.75, 60, 0.88, 2.75\n",
    "    measured extent of the magnitude type ~ only used for wind speeds and hail size (e.g. 0.75” of hail; 60 mph winds)\n",
    "\n",
    "    magnitude_type   Ex: EG, MS, MG, ES\n",
    "    EG = Wind Estimated Gust; ES = Estimated Sustained Wind; MS = Measured Sustained Wind; MG = Measured Wind Gust (no magnitude is included for instances of hail)\n",
    "\n",
    "    flood_cause  Ex:  Ice Jam, Heavy Rain, Heavy Rain/Snow Melt\n",
    "    Reported or estimated cause of the flood\n",
    "\n",
    "    category  Ex: \n",
    "    Unknown (During the time of downloading this particular file, NCDC has never seen anything provided within this field.)\n",
    "\n",
    "    tor_f_scale  Ex: EF0, EF1, EF2, EF3, EF4, EF5\n",
    "    Enhanced Fujita Scale describes the strength of the tornado based on the amount and type of damage caused by the tornado.  The F-scale of damage will vary in the destruction area; therefore, the highest value of the F-scale is recorded for each event. \n",
    "    EF0 – Light Damage (40 – 72 mph)\n",
    "    EF1 – Moderate Damage (73 – 112 mph)\n",
    "    EF2 – Significant damage (113 – 157 mph)\n",
    "    EF3 – Severe Damage (158 – 206 mph)\n",
    "    EF4 – Devastating Damage (207 – 260 mph)\n",
    "    EF5 – Incredible Damage (261 – 318 mph)\n",
    "\n",
    "    tor_length  Ex: 0.66, 1.05, 0.48\n",
    "    Length of the tornado or tornado segment while on the ground (minimal of tenths of miles)\n",
    "\n",
    "    tor_width  Ex:  25, 50, 2640, 10\n",
    "    Width of the tornado or tornado segment while on the ground (in feet)\n",
    "\n",
    "    tor_other_wfo  Ex: DDC, ICT, TOP,OAX\n",
    "    Indicates the continuation of a tornado segment as it crossed from one National Weather Service Forecast Office to another.  The subsequent WFO identifier is provided within this field\n",
    "\n",
    "    tor_other_cz_state   Ex: KS, NE, OK\n",
    "    The two character representation for the state name of the continuing tornado segment as it crossed from one county or zone to another.  The subsequent 2-Letter State ID is provided within this field.\n",
    "\n",
    "    tor_other_cz_fips  Ex: 41, 127, 153\n",
    "    The FIPS number of the county entered by the continuing tornado segment as it crossed from one county to another.  The subsequent FIPS number is provided within this field.\n",
    "\n",
    "    tor_other_cz_name  Ex: DICKINSON, NEMAHA, SARPY\n",
    "    The FIPS name of the county entered by the continuing tornado segment as it crossed from one county to another.  The subsequent county or zone name is provided within this field in ALL CAPS.\n",
    "\n",
    "    episode_title  Ex: Severe weather outbreak on Saturday April 14 in eastern Nebraska\n",
    "    A short description of the episode.  (Short name for the episode itself as determined by NWS.)\n",
    "\n",
    "    episode_narrative  (The episode narrative depicting the general nature and overall activity of the episode.  The narrative is created by NWS.)  Ex: A strong upper level system over the southern Rockies lifted northeast across the plains causing an intense surface low pressure system and attendant warm front to lift into Nebraska.  \n",
    "\n",
    "    event_narrative  (The event narrative provides more specific details of the individual event .  The event narrative is provided by NWS.) Ex:  Heavy rain caused flash flooding across parts of Wilber.  Rainfall of 2 to 3 inches fell across the area. \n",
    "\n",
    "    Storm Data Location File (named event_locations_YYYYMM.csv):\n",
    "    episode_id   Ex: 60904 (ID assigned by NWS to denote the storm episode; links the storm episode with the information within the event details file) An Episode may contain several different events.\n",
    "    event_id     Ex: 364000, 364001, 364002, 364003 (ID assigned by NWS to note a single, small part that goes into a specific storm episode; links the storm episode between the three files downloaded from SPC’s website)\n",
    "    location_index   Ex: 1-x (Number assigned  by NWS to specific locations within the same Storm event. Each event’s sequentially increasing location index number will have a corresponding lat/lon point)\n",
    "    range   Ex: 0.59, 0.69, 4.84, 1.17 {A hydro-meteorological event will be referenced, minimally, to the nearest tenth of a mile, to the geographical center (not from the village/city boundaries or limits) of a particular village/city, airport, or inland lake, providing that the reference point is documented in the Storm Data software location database. }\n",
    "    azimuth  Ex: ENE, NW, WSW, S  {16-point compass direction from a particular village/city, airport, or inland lake, providing that the reference point is documented in the Storm Data software location database of > 130,000 locations.}\n",
    "    location  Ex:  PINELAND,  CENTER, ORRS, RUSK {center from which the range is calculated and the azimuth is determined}\n",
    "\n",
    "    lat  Ex: 31.25, 31.79, 32.76, 31.80\n",
    "    The latitude where the event occurred {rounded to the hundredths in decimal degrees; includes an ‘-‘ if it’s S of the Equator}\n",
    "\n",
    "    lon  Ex: -93.97, -94.18, -94.52, -95.13\n",
    "    The longitude where the event occurred {rounded to the hundredths in decimal degrees; includes an ‘-‘ if it’s W of the Prime Meridian}\n",
    "\n",
    "    Storm Data Fatality File (named event_fatalities_YYYYMM.csv):\n",
    "    fatality_id  Ex: 17582, 17590, 17597, 18222 (ID assigned by NWS to denote the individual fatality that occurred within a storm event)\n",
    "    event_id  Ex: 364302, 365560, 365945, 367330 (ID assigned by NWS to note a single, small part that goes into a specific storm episode; links the storm episode between the three files downloaded from SPC’s website)\n",
    "    fatality_type  Ex: D , I (D = Direct Fatality; I = Indirect Fatality; assignment of this is determined by NWS software; details below are from NWS Directve 10-1605 at http://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf, Section 2.6)\n",
    "        •\tDirect Fatality:  “A direct fatality or injury is defined as a fatality or injury directly attributable to the hydro-meteorological event itself, or impact by airborne/falling/moving debris, i.e., missiles generated by wind, water, ice, lightning, tornado, etc. “\n",
    "        •\tIndirect Fatality:  “Fatalities and injuries, occurring in the vicinity of a hydrometeorological event, or after it has ended, but not directly caused by impact or debris from the event (weather event was a passive entity), are classified as indirect.”\n",
    "\n",
    "    fatality_date  Ex: 4/3/2012  12:00:00 AM, 4/11/2012  12:00:00 AM\n",
    "    MM/DD/YYYY  12 hour time AM/PM\n",
    "\n",
    "    fatality_age  Ex: 38, 25, 69, 54\n",
    "    The age of the fatalities\n",
    "\n",
    "    fatality_sex  Ex: M, F\n",
    "    The gender of the fatalities\n",
    "\n",
    "    fatality_location  Ex: Under Tree, Boating, Vehicle/Towed Trailer\n",
    "\n",
    "    Direct Fatality Location Table\n",
    "\n",
    "    BF Ball Field\n",
    "    BO Boating\n",
    "    BU Business\n",
    "    CA Camping\n",
    "    CH Church\n",
    "    EQ Heavy Equip/Construction\n",
    "    GF Golfing\n",
    "    IW In Water\n",
    "    LS Long Span Roof\n",
    "    MH Mobile/Trailer Home\n",
    "    OT Other/Unknown\n",
    "    OU Outside/Open Areas\n",
    "    PH Permanent Home\n",
    "    PS Permanent Structure\n",
    "    SC School\n",
    "    TE Telephone\n",
    "    UT Under Tree\n",
    "    VE Vehicle and/or Towed Trailer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Thunderstorm Wind           239820\n",
       "Hail                        230213\n",
       "Flash Flood                  60548\n",
       "Winter Storm                 53098\n",
       "High Wind                    49553\n",
       "Heavy Snow                   47711\n",
       "Drought                      43316\n",
       "Flood                        36731\n",
       "Winter Weather               30065\n",
       "Tornado                      24920\n",
       "Heat                         16523\n",
       "Marine Thunderstorm Wind     15366\n",
       "Heavy Rain                   15217\n",
       "Strong Wind                  14704\n",
       "Lightning                    14229\n",
       "Cold/Wind Chill              10494\n",
       "Blizzard                     10299\n",
       "Ice Storm                     9317\n",
       "Dense Fog                     8478\n",
       "Frost/Freeze                  8349\n",
       "Funnel Cloud                  6648\n",
       "Extreme Cold/Wind Chill       6481\n",
       "High Surf                     5969\n",
       "Excessive Heat                5805\n",
       "WINTER WEATHER                5697\n",
       "Wildfire                      5233\n",
       "Tropical Storm                3933\n",
       "Waterspout                    3818\n",
       "Hurricane (Typhoon)           1758\n",
       "Coastal Flood                 1676\n",
       "Lake-Effect Snow              1371\n",
       "Storm Surge/Tide              1157\n",
       "Rip Current                    897\n",
       "Dust Storm                     701\n",
       "Marine Hail                    539\n",
       "Sleet                          505\n",
       "Avalanche                      460\n",
       "Debris Flow                    460\n",
       "Landslide                      362\n",
       "Tropical Depression            313\n",
       "Astronomical Low Tide          302\n",
       "Freezing Fog                   273\n",
       "Marine High Wind               236\n",
       "Dust Devil                     171\n",
       "Marine Strong Wind              76\n",
       "Volcanic Ash                    74\n",
       "Seiche                          42\n",
       "Lakeshore Flood                 41\n",
       "Tsunami                         29\n",
       "Hurricane                       19\n",
       "Dense Smoke                     18\n",
       "Northern Lights                  8\n",
       "Marine Tropical Storm            6\n",
       "Heavy Wind                       4\n",
       "Sneakerwave                      3\n",
       "Marine Dense Fog                 1\n",
       "High Snow                        1\n",
       "OTHER                            1\n",
       "Name: EVENT_TYPE, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.EVENT_TYPE.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to combine a few dataSeries to get a date\n",
    "df1['begin_date_full'] = pd.to_datetime(df1.BEGIN_YEARMONTH*100+df1.BEGIN_DAY,format='%Y%m%d',errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Want to add a column with a combination of injuries and deaths called casualties\n",
    "df1['casualties'] = pd.DataFrame(df1.INJURIES_DIRECT + df1.INJURIES_INDIRECT + df1.DEATHS_DIRECT + df1.DEATHS_INDIRECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# I think I may dummy some of these columns\n",
    "df1['EVENT_TYPE'] = df1['EVENT_TYPE'].astype('category')\n",
    "df1['STATE'] = df1['STATE'].astype('category')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I may want to extract just a few key columns that I will need\n",
    "data = df1[['begin_date_full', 'STATE', 'EVENT_TYPE', 'INJURIES_DIRECT', \n",
    "            'INJURIES_INDIRECT', 'DEATHS_DIRECT', 'DEATHS_INDIRECT',\n",
    "            'casualties','DAMAGE_PROPERTY', 'DAMAGE_CROPS']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "begin_date_full      datetime64[ns]\n",
       "STATE                      category\n",
       "EVENT_TYPE                 category\n",
       "INJURIES_DIRECT               int64\n",
       "INJURIES_INDIRECT             int64\n",
       "DEATHS_DIRECT                 int64\n",
       "DEATHS_INDIRECT               int64\n",
       "casualties                    int64\n",
       "DAMAGE_PROPERTY              object\n",
       "DAMAGE_CROPS                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    994039.000000\n",
       "mean          0.087061\n",
       "std           4.036967\n",
       "min           0.000000\n",
       "25%           0.000000\n",
       "50%           0.000000\n",
       "75%           0.000000\n",
       "max        2409.000000\n",
       "Name: casualties, dtype: float64"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.casualties.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count     541266\n",
       "unique      2384\n",
       "top        0.00K\n",
       "freq      267264\n",
       "Name: DAMAGE_PROPERTY, dtype: object"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.DAMAGE_PROPERTY.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Series.unique of 0           NaN\n",
      "1           NaN\n",
      "2           NaN\n",
      "3           NaN\n",
      "4           NaN\n",
      "5           NaN\n",
      "6           NaN\n",
      "7           NaN\n",
      "8           NaN\n",
      "9           NaN\n",
      "10          NaN\n",
      "11          NaN\n",
      "12          NaN\n",
      "13          NaN\n",
      "14          NaN\n",
      "15          NaN\n",
      "16          NaN\n",
      "17          NaN\n",
      "18          NaN\n",
      "19          NaN\n",
      "20          NaN\n",
      "21          NaN\n",
      "22          NaN\n",
      "23          NaN\n",
      "24          NaN\n",
      "25          NaN\n",
      "26          NaN\n",
      "27          NaN\n",
      "28          NaN\n",
      "29          NaN\n",
      "          ...  \n",
      "994009    0.00K\n",
      "994010    0.00K\n",
      "994011      NaN\n",
      "994012    0.00K\n",
      "994013    0.00K\n",
      "994014    0.00K\n",
      "994015    0.00K\n",
      "994016    0.00K\n",
      "994017    0.00K\n",
      "994018    0.00K\n",
      "994019    0.00K\n",
      "994020    0.00K\n",
      "994021    0.00K\n",
      "994022    0.00K\n",
      "994023    0.00K\n",
      "994024    0.00K\n",
      "994025    0.00K\n",
      "994026    0.00K\n",
      "994027    0.00K\n",
      "994028    0.00K\n",
      "994029    0.00K\n",
      "994030    0.00K\n",
      "994031    0.00K\n",
      "994032    0.00K\n",
      "994033    0.00K\n",
      "994034    0.00K\n",
      "994035    0.00K\n",
      "994036    0.00K\n",
      "994037    0.00K\n",
      "994038    0.00K\n",
      "Name: DAMAGE_CROPS, dtype: object>\n"
     ]
    }
   ],
   "source": [
    "print data.DAMAGE_CROPS.unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-68-ff60a3f34536>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m data.DAMAGE_CROPS = (data.DAMAGE_CROPS.replace(r'[KMB]+$', '', regex=True).astype(float) *                         data.DAMAGE_CROPS.str.extract(r'[\\d\\.]+([KMB]+)', expand=False)\n\u001b[0m\u001b[1;32m      2\u001b[0m                         \u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m                         .replace(['K','M','B'], [10**3, 10**6, 10**9]).astype(int))\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, raise_on_error, **kwargs)\u001b[0m\n\u001b[1;32m   2948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2949\u001b[0m         mgr = self._data.astype(dtype=dtype, copy=copy,\n\u001b[0;32m-> 2950\u001b[0;31m                                 raise_on_error=raise_on_error, **kwargs)\n\u001b[0m\u001b[1;32m   2951\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmgr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, **kwargs)\u001b[0m\n\u001b[1;32m   2936\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2937\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2938\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'astype'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2940\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, f, axes, filter, do_integrity_check, consolidate, raw, **kwargs)\u001b[0m\n\u001b[1;32m   2888\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2889\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mgr'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2890\u001b[0;31m             \u001b[0mapplied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2891\u001b[0m             \u001b[0mresult_blocks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_extend_blocks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mapplied\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult_blocks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mastype\u001b[0;34m(self, dtype, copy, raise_on_error, values, **kwargs)\u001b[0m\n\u001b[1;32m    432\u001b[0m                **kwargs):\n\u001b[1;32m    433\u001b[0m         return self._astype(dtype, copy=copy, raise_on_error=raise_on_error,\n\u001b[0;32m--> 434\u001b[0;31m                             values=values, **kwargs)\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m     def _astype(self, dtype, copy=False, raise_on_error=True, values=None,\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36m_astype\u001b[0;34m(self, dtype, copy, raise_on_error, values, klass, mgr, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m                 \u001b[0;31m# _astype_nansafe works fine with 1-d only\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_astype_nansafe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ugp/anaconda2/lib/python2.7/site-packages/pandas/core/common.pyc\u001b[0m in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy)\u001b[0m\n\u001b[1;32m   1918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1919\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1920\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1921\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: "
     ]
    }
   ],
   "source": [
    "data.DAMAGE_CROPS = (data.DAMAGE_CROPS.replace(r'[KMB]+$', '', regex=True).astype(float) * \\\n",
    "                        data.DAMAGE_CROPS.str.extract(r'[\\d\\.]+([KMB]+)', expand=False)\n",
    "                        .fillna(1)\n",
    "                        .replace(['K','M','B'], [10**3, 10**6, 10**9]).astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus: Explain how you intend to evaluate your results. \n",
    "What tuning metric and evaluation approaches do you intend to use?\n",
    "Clean> fit> run models, look at the results... \n",
    "\n",
    "Linear regression: Attempt to accurately predict the independent variable (casualities) by good fitting SSE / differential \n",
    "\n",
    "Logistic regression: Attempt to predict if I can say if a certain storm in a certain state, in a certain month would cause a causality. I would show classification accuracy score, confusion matrix coorelation number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus Identify 1-2 additional datasets that may help you triangulate your findings. \n",
    "How might these relate to your data?\n",
    "\n",
    "Could look at the aircraft incident reports from the NTSB site\n",
    "May get an idea of crop damage in current dollar basis, perhaps rebase for inflation or the proper crop price in todays dollars, crop price website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "\n",
    "- [Exploratory Data Analysis](http://insightdatascience.com/blog/eda-and-graphics-eli-bressert.html)\n",
    "- [Best practices for data documentation](https://www.dataone.org/all-best-practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
