{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataframe construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df1 = pd.read_csv('../Part 2 Capstone/df2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df1['MONTH_NAME'] = df1['MONTH_NAME'].apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " event_types = ['Drought' ,'Flood' ,'Winter Storm', 'OTHER', 'High Wind' ,'ThunderStorm',\n",
    " 'Wildfire', 'Rip Current' ,'Tropical Storm' ,'Tornado' ,'Hurricane', 'Tsunami',\n",
    " 'Landslide']\n",
    " for i in event_types:\n",
    "     df1['is_'+i] = df1['EVENT_TYPE'].apply(lambda x: 1 if i in x else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['COLORADO', 'TEXAS', 'CALIFORNIA', 'NEW MEXICO', 'GEORGIA',\n",
       "       'FLORIDA', 'OREGON', 'ARIZONA', 'UTAH', 'WASHINGTON', 'IDAHO',\n",
       "       'PENNSYLVANIA', 'NEBRASKA', 'KANSAS', 'INDIANA', 'ILLINOIS',\n",
       "       'AMERICAN SAMOA', 'NEW JERSEY', 'VIRGINIA', 'MARYLAND', 'KENTUCKY',\n",
       "       'MISSOURI', 'VERMONT', 'NEW YORK', 'MASSACHUSETTS',\n",
       "       'GULF OF MEXICO', 'MONTANA', 'WEST VIRGINIA', 'DELAWARE',\n",
       "       'MICHIGAN', 'OHIO', 'HAWAII', 'ATLANTIC SOUTH', 'ALASKA',\n",
       "       'NEW HAMPSHIRE', 'RHODE ISLAND', 'CONNECTICUT', 'NORTH CAROLINA',\n",
       "       'MAINE', 'SOUTH DAKOTA', 'TENNESSEE', 'DISTRICT OF COLUMBIA',\n",
       "       'NEVADA', 'WISCONSIN', 'OKLAHOMA', 'IOWA', 'ARKANSAS',\n",
       "       'SOUTH CAROLINA', 'NORTH DAKOTA', 'WYOMING', 'PUERTO RICO',\n",
       "       'ALABAMA', 'MISSISSIPPI', 'MINNESOTA', 'LOUISIANA', 'GUAM',\n",
       "       'ATLANTIC NORTH', 'E PACIFIC', 'HAWAII WATERS', 'VIRGIN ISLANDS',\n",
       "       'LAKE MICHIGAN', 'LAKE ERIE', 'LAKE ST CLAIR', 'LAKE HURON',\n",
       "       'LAKE ONTARIO', 'ST LAWRENCE R', 'LAKE SUPERIOR', nan,\n",
       "       'GULF OF ALASKA'], dtype=object)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.STATE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(654313, 25)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Below i will construct a number of dataframes that I will subsequently run both regression and classification models on.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(df1.groupby('EVENT_TYPE').CASUALTIES.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for column in df1.columns:\n",
    "    print df1[column].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sb\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.cross_validation import train_test_split \n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression()\n",
    "\n",
    "y = LabelEncoder().fit_transform(df1['is_Hurricane'])\n",
    "x = df1['TOTAL_DEATHS'].reshape(-1,1)\n",
    "model_1 = logmodel.fit(x,y)\n",
    "\n",
    "model_1.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# setting up train and test data for X and y\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# data is shuffled\n",
    "# cv = KFold(len(df1),n_folds=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# stratified and shuffled these are the ones for train and test set\n",
    "# cv = StratifiedKFold(y, n_folds=3, shuffle=True, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import seaborn as sbn\n",
    "plt.figure(figsize=(11,8))\n",
    "sbn.heatmap(df1.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# global_modelswant to build a global dict to store all my models for future use\n",
    "#  = {\n",
    "#             'bdt': BaggingClassifier(DecisionTreeClassifier()), \n",
    "#             'rf': RandomForestClassifier(class_weight='balanced', n_jobs=-1),\n",
    "#             'et': ExtraTreesClassifier(class_weight='balanced', n_jobs=-1)\n",
    "# }    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "global_models = {\n",
    "   'knn' : KNeighborsClassifier(),\n",
    "   'knn_bag' : BaggingClassifier(KNeighborsClassifier()),\n",
    "   'logreg' : LogisticRegression(),\n",
    "   'logreg_bag' : BaggingClassifier(LogisticRegression()),\n",
    "   'tree' : DecisionTreeClassifier(),\n",
    "   'tree_bag' : BaggingClassifier(DecisionTreeClassifier()),\n",
    "   'forest' : RandomForestClassifier(),\n",
    "   'forest_bag' : BaggingClassifier(RandomForestClassifier()),\n",
    "   'extra_tree' : ExtraTreesClassifier(),\n",
    "   'extra_tree_bag' : BaggingClassifier(ExtraTreesClassifier())\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "\n",
    "def evaluate_model(X_train, X_test, \n",
    "                   y_train, y_test,\n",
    "                  model):\n",
    "    pick_a_model = global_models[model]\n",
    "    pick_a_model.fit(X_train, y_train)\n",
    "    print 'train model score:', pick_a_model.score(X_train, y_train)\n",
    "    predictions = pick_a_model.predict(X_test)\n",
    "    print '\\nconfusion matrix: \\n\\n', confusion_matrix(y_test, predictions)\n",
    "    print '\\nclassification report: \\n\\n', classification_report(y_test, predictions)\n",
    "    results.update({model: pick_a_model.score(X_test, y_test)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for key in global_models:\n",
    "    evaluate_model(X_train, X_test, y_train, y_test, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 Most Combined Damage: CALIFORNIA\t2006\tJanuary\tFlood\n",
    "CD1 = df1.groupby('EPISODE_ID').COMBINED_DAMAGE.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11)\n",
    "CD1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# MOST INJURIES- #1 TEXAS\t1998\tOctober\tFlood\tThe Great October Flood\n",
    "I1 = df1.groupby('EPISODE_ID').TOTAL_INJURIES.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11)\n",
    "I1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 TOTAL_DEATHS: MISSOURI\t2011\tMay\tTornado - A strong upper level trough across the central..\n",
    "D1 = df1.groupby('EPISODE_ID').TOTAL_DEATHS.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11)\n",
    "D1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Most Casualties- #1 TEXAS\t1998\tOctober\tFlood\tThe Great October Flood\n",
    "\n",
    "X1 = df1.groupby('EPISODE_ID').CASUALTIES.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11)\n",
    "X1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 TEXAS\t1998\tOctober\tFlood\tThe Great October Flood\n",
    "df1[df1.EPISODE_ID == 1150299]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2 TEXAS\t2008\tSeptember\tHurricane IKE - The eye of Hurricane Ike moved ashore in Galve...\n",
    "df1[df1.EPISODE_ID == 23202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#3 ALABAMA\t2011\tApril\tTornado: A powerful storm system crossed the Southeast \n",
    "df1[df1.EPISODE_ID == 50455]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#4 MISSOURI\t2011\tMay\tTornado - A strong upper level trough across the central..\n",
    "df1[df1.EPISODE_ID == 49972]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#5 \tMISSOURI\t2007\tAugust\tDrought The first and only Heat Wave of the summer sta...\n",
    "df1[df1.EPISODE_ID == 10217]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#6 FLORIDA\t2004\tAugust\tHurricane The collective effects of Hurricane Charley\n",
    "df1[df1.EPISODE_ID == 178038]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#7 TEXAS\t1998\tOctober\tFlood Flooding along the Guadalupe Rive\n",
    "df1[df1.EPISODE_ID == 1150309]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#8 OKLAHOMA\t1999\tMay\tTornado Rocks Oklahoma - A record outbreak of tornadoes struck Oklaho.\n",
    "df1[df1.EPISODE_ID == 82875]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#9 TEXAS\t1998\tOctober\tFlood - Flooding along the San Antonio River\n",
    "df1[df1.EPISODE_ID == 70489]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#10 UTAH\t2013\tJanuary\tWinter Storm A rare freezing rain event\n",
    "df1[df1.EPISODE_ID == 71079]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#11 TENNESSEE\t2011\tApril\tWinter Storm\n",
    "df1[df1.EPISODE_ID == 49915]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 \tTEXAS\t1998\tOctober\tFlood\n",
    "df1[df1.EPISODE_ID == 198565].TOTAL_DEATHS.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1 = df1.groupby('EVENT_TYPE').CASUALTIES.agg(['max']).sort_values('max', axis=0, ascending=False).reset_index().head(10)\n",
    "a1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1[(df1.CASUALTIES == 2409) & (df1.EVENT_TYPE == 'Hurricane') | \n",
    "    (df1.CASUALTIES == 1311) & (df1.EVENT_TYPE == 'Tornado') | \n",
    "    (df1.CASUALTIES == 802) & (df1.EVENT_TYPE == 'Flood') |\n",
    "    (df1.CASUALTIES == 521) & (df1.EVENT_TYPE == 'Drought') | \n",
    "    (df1.CASUALTIES == 300) & (df1.EVENT_TYPE == 'Winter Storm') | \n",
    "    (df1.CASUALTIES == 180) & (df1.EVENT_TYPE == 'Tsunami') | \n",
    "    (df1.CASUALTIES == 104) & (df1.EVENT_TYPE == 'Wildfire') | \n",
    "    (df1.CASUALTIES == 101) & (df1.EVENT_TYPE == 'High Wind')       \n",
    "   ].sort_values('CASUALTIES', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c1 = df1.groupby('EVENT_TYPE').TOTAL_DEATHS.agg(['max']).sort_values('max', axis=0, ascending=False).reset_index().head(10)\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1[(df1.TOTAL_DEATHS == 161) & (df1.EVENT_TYPE == 'Tornado') | \n",
    "    (df1.TOTAL_DEATHS == 93) & (df1.EVENT_TYPE == 'Drought') | \n",
    "    (df1.TOTAL_DEATHS == 32) & (df1.EVENT_TYPE == 'Tsunami') |\n",
    "    (df1.TOTAL_DEATHS == 23) & (df1.EVENT_TYPE == 'Flood') | \n",
    "    (df1.TOTAL_DEATHS == 22) & (df1.EVENT_TYPE == 'Tropical Storm') | \n",
    "    (df1.TOTAL_DEATHS == 19) & (df1.EVENT_TYPE == 'Heavy Rain') | \n",
    "    (df1.TOTAL_DEATHS == 19) & (df1.EVENT_TYPE == 'Wildfire') \n",
    "   ].sort_values('TOTAL_DEATHS', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b1 = df1.groupby('EVENT_TYPE').TOTAL_INJURIES.agg(['max']).sort_values('max', axis=0, ascending=False).reset_index().head(10)\n",
    "b1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1[(df1.TOTAL_INJURIES == 2400) & (df1.EVENT_TYPE == 'Hurricane') | \n",
    "    (df1.TOTAL_INJURIES == 1150) & (df1.EVENT_TYPE == 'Tornado') | \n",
    "    (df1.TOTAL_INJURIES == 800) & (df1.EVENT_TYPE == 'Flood') |\n",
    "    (df1.TOTAL_INJURIES == 519) & (df1.EVENT_TYPE == 'Drought') | \n",
    "    (df1.TOTAL_INJURIES == 300) & (df1.EVENT_TYPE == 'Winter Storm') | \n",
    "    (df1.TOTAL_INJURIES == 154) & (df1.EVENT_TYPE == 'Tropical Storm') | \n",
    "    (df1.TOTAL_INJURIES == 100) & (df1.EVENT_TYPE == 'High Wind') |\n",
    "    (df1.TOTAL_INJURIES == 148) & (df1.EVENT_TYPE == 'Tsunami') | \n",
    "    (df1.TOTAL_INJURIES == 90) & (df1.EVENT_TYPE == 'Wildfire') \n",
    "   ].sort_values('TOTAL_INJURIES', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "d1 = df1.groupby('EVENT_TYPE').DAMAGE_PROPERTY.agg(['max']).sort_values('max', axis=0, ascending=False).reset_index().head(10)\n",
    "d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1[(df1.DAMAGE_PROPERTY == 1.150000e+11) & (df1.EVENT_TYPE == 'Flood') | \n",
    "    (df1.DAMAGE_PROPERTY == 1.790000e+10) & (df1.EVENT_TYPE == 'Rip Current') | \n",
    "    (df1.DAMAGE_PROPERTY == 1.000000e+10) & (df1.EVENT_TYPE == 'Hurricane') |\n",
    "    (df1.DAMAGE_PROPERTY == 2.800000e+09) & (df1.EVENT_TYPE == 'Tornado') | \n",
    "    (df1.DAMAGE_PROPERTY == 1.800000e+09) & (df1.EVENT_TYPE == 'Winter Storm') | \n",
    "    (df1.DAMAGE_PROPERTY == 1.500000e+09) & (df1.EVENT_TYPE == 'High Wind') | \n",
    "    (df1.DAMAGE_PROPERTY == 1.500000e+09) & (df1.EVENT_TYPE == 'WildFire') |\n",
    "    (df1.DAMAGE_PROPERTY == 2.500000e+08) & (df1.EVENT_TYPE == 'Drought') | \n",
    "    (df1.DAMAGE_PROPERTY == 8.100000e+07) & (df1.EVENT_TYPE == 'Tsunami') \n",
    "   ].sort_values('DAMAGE_PROPERTY', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
