{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JayMichChi Part 2 Capstone\n",
    "### Goal: Jays summary notebook outlining my project's goals, methods, models, and EDA.\n",
    "\n",
    "This will be an overview of my approach with a well-articulated summary that includes your problem statement, outlines your proposed methods and models, defines any risks & assumptions, and includes any revisions from your initial goals & criteria, as needed.\n",
    "\n",
    "In addition, I will explain how I intend to store my data. \n",
    "\n",
    "You'll want to describe and create a data dictionary for your dataset(s), and perform your initial exploratory data analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### My hypothesis will attempt to:\n",
    "Reasonably predict the amount of annual deaths and property damage due to certain types of storm occurrences based on state and storm type past data.\n",
    "\n",
    "### Capstone Project\n",
    "My capstone will be gathering data from the National Centers for Environmental Information, particularly information from 2003 to 2013, and will look at unusual storm occurrences and unusual weather phenomena, and mores specifically, information on storm paths, deaths, injuries, and property damage.\n",
    "\n",
    "### Outline of Proposed Data Collection Methods \n",
    "National Centers for Environmental Information (NCEI) is responsible for hosting and providing access to one of the most significant archives on Earth, with comprehensive oceanic, atmospheric, and geophysical data. From the depths of the ocean to the surface of the sun and from million-year-old sediment records to near real-time satellite images, NCEI is the Nation’s leading authority for environmental information.  It is from this source that I will primarily ingest data on the following: https://www.ncdc.noaa.gov/stormevents/\n",
    "\n",
    "### Storm Events Database\n",
    "The Storm Events Database contains a chronological listing, by states, of storm occurrences and unusual weather phenomena. Reports contain information on storm paths, deaths, injuries, and property damage.s\n",
    "\n",
    "The Storm Events Database currently contains data from January 1950 to August 2016, as entered by NOAA's National Weather Service (NWS). Due to changes in the data collection and processing procedures over time, there are unique periods of record available depending on the event type. I have chosen to look at the data from 1996 to 2013.\n",
    "\n",
    "### Data base\n",
    "I was able to find the 10 years of data on the following NCDC website FTP server. \n",
    "ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/\n",
    "\n",
    "    Index of /pub/data/swdi/stormevents/csvfiles/legacy/\n",
    "    Name\tSize\tDate Modified\n",
    "    ugc_areas.csv\t902 kB\t2/18/14, 12:00:00 AM\n",
    "    stormdata_2013.csv\t53.1 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2012.csv\t64.3 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2011.csv\t82.3 MB\t1/31/14, 12:00:00 AM\n",
    "    stormdata_2010.csv\t62.5 MB\t1/31/14, 12:00:00 AM\n",
    "    Stormdata_2009.csv\t49.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2008.csv\t60.6 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2007.csv\t49.7 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2006.csv\t26.0 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2005.csv\t34.9 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2004.csv\t33.1 MB\t9/19/13, 12:00:00 AM\n",
    "    Stormdata_2003.csv\t32.1 MB\t9/19/13, 12:00:00 AM\n",
    "  \n",
    "I have successfully downloaded all 10 CSV files. My work will show that i merged the data via pandas concant processes. Additionally, I will have to clean the data, reformat dates, categorize certain feature series, and perhaps use dummy variables to allow me to model the data successfully. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Proposed Data Cleaning Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Risks & Assumptions\n",
    "\n",
    "There are some risks and assumptions that I need to incorporate into my capstone and the data. \n",
    "\n",
    "First, I will be looking at the dollar damage. Certainly inflation will come into play as the cost of crops and property replacement costs fluctuate. This is one of the primary reasons that I am reducing my data to just 10 years. \n",
    "\n",
    "(The other reason is the sheer size of the database) \n",
    "\n",
    "There are also confounding factors that will effect the results: For instance, absolutely/categorically attributing  fatalities to a weather event may not be completely correct, and may in fact occcur our of coincidence. Additionally, there are extenuating factors that could effect the cost or fatality number. For instance, the responsiveness of the National Guard, or massive federal funding may effect the amount of fatalities, or the extent of damage. \n",
    "\n",
    "In short, the assumption we make is that all these extenuating factors will not adversely effect the results because our dataset of over 600,000 instances is so large. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outline of Proposed Data Modelling\n",
    "\n",
    "I expect that once i complete cleaning the data set, I will then fitting this data to a few modeles.\n",
    "\n",
    "My initial inclination is to see if I can find any coorelations in the 58 columns of data. \n",
    "\n",
    "I believe that Logistic Regression model and Linear Regression models will be the first two models I will run. \n",
    "\n",
    "\n",
    "I will perform a train test split on just a segment of random data I grab from my database. This will allow me to work with dataset that is more managable for my laptop. \n",
    "\n",
    "Once cleaned, tts, fit and run, I will attempt to graph the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create local database\n",
    "Below I grab my data from NOAA "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#here is my scroll. it works, but the database url_base changes in y2010, \n",
    "# so i will re-run this with the updated base_url\n",
    "import pandas as pd\n",
    "import time\n",
    "url_list = ['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_2003.csv']\n",
    "web_count= 2003\n",
    "for pages in range(1,7):\n",
    "    web_count= 1 + web_count\n",
    "    count_update=str(web_count)\n",
    "    base_url = 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/Stormdata_' + count_update + '.csv'\n",
    "    url_list.append(base_url)\n",
    "url_list\n",
    "# This works wonderfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is my one time function that will load, append\n",
    "# for url in url_list:\n",
    "data1 = []\n",
    "for url in url_list:\n",
    "    page = pd.read_csv((url)) \n",
    "    data1.append(page)\n",
    "    time.sleep(3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data1_all = pd.concat(data1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print data1_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#as above\n",
    "# so i will re-run this with the updated base_url\n",
    "\n",
    "import time\n",
    "url_list_2 = ['ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_2010.csv']\n",
    "web_count= 2010\n",
    "for pages in range(1,4):\n",
    "    web_count= 1 + web_count\n",
    "    count_update=str(web_count)\n",
    "    base_url_2 = 'ftp://ftp.ncdc.noaa.gov/pub/data/swdi/stormevents/csvfiles/legacy/stormdata_' + count_update + '.csv'\n",
    "    url_list_2.append(base_url_2)\n",
    "url_list_2\n",
    "# This works wonderfully!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# This is my one time function that will load, append\n",
    "# for url in url_list:\n",
    "import pandas as pd\n",
    "data2 = []\n",
    "for url in url_list_2:\n",
    "    page = pd.read_csv((url)) \n",
    "    data2.append(page)\n",
    "    time.sleep(30.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data2_all=pd.concat(data2, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data2_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat([data1_all, data2_all], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert dataframe to csv\n",
    "\n",
    "df_all.to_csv('jm_cap.csv')\n",
    "#This worked!  Thank God this was successful on Tuesday school!\n",
    "# 18 years worth of data (654313, 58)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### DONE DATABASE IS SUCCESSFULLY DOWNLOADED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a data dictionary\n",
    "\n",
    "    Storm Data Export Format, Field names\n",
    "\n",
    "    Event Details File (named event_details_YYYYMM.csv):\n",
    "    last_date_modified  Ex:  4/4/2012  11:05:44 PM, 5/3/2012  5:10:49 AM\n",
    "    MM/DD/YYYY  12 hour time AM/PM\n",
    "\n",
    "    The last date of modification by NWS.  Any corrections to the storm event/episode in question are made solely by NWS and the person that actually entered the event/episode.\n",
    "\n",
    "    last_date_certified  Ex:  5/10/2012  9:10:51 AM, 5/18/2012  12:21:15 AM \n",
    "    MM/DD/YYYY  12 hour time AM/PM\n",
    "\n",
    "    The last date of certification by NWS.  Any corrections to the storm event/episode in question are made solely by NWS and the person that actually entered the event/episode.\n",
    "\n",
    "    episode_id   Ex: 61280, 62777, 63250\n",
    "    (ID assigned by NWS to denote the storm episode; links the event details file with the information within location file)\n",
    "\n",
    "    The occurrence of storms and other significant weather phenomena having sufficient intensity to cause loss of life, injuries, significant property damage, and/or disruption to commerce.  Rare, unusual, weather phenomena that generate media attention, such as snow flurries in South Florida or the San Diego coastal area; and Other significant meteorological events, such as record maximum or minimum temperatures or precipitation that occur in connection with another event.\n",
    "\n",
    "    event_id  Ex: 383097, 374427, 364175\n",
    "\n",
    "    (Primary database key field)\n",
    "    (ID assigned by NWS to note a single, small part that goes into a specific storm episode; links the storm episode between the three files downloaded from SPC’s website)\n",
    "\n",
    "    state  Ex:  GEORGIA,  WYOMING, COLORADO\n",
    "    The state name where the event occurred (no State ID’s are included here; State Name is spelled out in ALL CAPS)\n",
    "\n",
    "    state_fips  Ex: 45, 30, 12\n",
    "    A unique number (State Federal Information Processing Standard) is assigned to the county by the National Institute for Standards and Technology (NIST).\n",
    "\n",
    "    year  Ex: 2000, 2006, 2012\n",
    "    Four digit year for the event in this record\n",
    "\n",
    "\n",
    "\n",
    "    month_name  Ex: January, February, March\n",
    "    Name of the month for the event in this record (spelled out; not abbreviated)\n",
    "\n",
    "    event_type  Ex: Hail, Thunderstorm Wind, Snow, Ice (spelled out; not abbreviated)\n",
    "\n",
    "    The only events permitted in Storm Data are listed in Table 1 of Section 2.1.1 of NWS Directive 10-1605 at http://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf. The chosen event name should be the one that most accurately describes the meteorological event leading to fatalities, injuries, damage, etc. However, significant events, such as tornadoes, having no impact or causing no damage, should also be included in Storm Data.\n",
    "\n",
    "    From Section 2.1.1 of NWS Directive 10-1605:\n",
    "    Event Name / Designator (County or Zone and whether the event happened in a (C) county/parish,(Z)zone or(M) marine\n",
    "    Astronomical Low Tide\tZ \n",
    "    Avalanche    Z \n",
    "    Blizzard    Z \n",
    "    Coastal Flood   Z \n",
    "    Cold/Wind Chill   Z \n",
    "    Debris Flow    C \n",
    "    Dense Fog    Z \n",
    "    Dense Smoke   Z \n",
    "    Drought    Z \n",
    "    Dust Devil    C \n",
    "    Dust Storm    Z \n",
    "    Excessive Heat   Z \n",
    "    Extreme Cold/Wind Chill   Z \n",
    "    Flash Flood    C   \n",
    "    Flood     C \n",
    "    Frost/Freeze    Z \n",
    "    Funnel Cloud   C \n",
    "    Freezing Fog   Z \n",
    "    Hail     C \n",
    "    Heat      Z \n",
    "    Heavy Rain    C \n",
    "    Heavy Snow    Z \n",
    "    High Surf    Z \n",
    "    High Wind    Z \n",
    "    Hurricane (Typhoon)   Z \n",
    "    Ice Storm    Z \n",
    "    Lake-Effect Snow   Z \n",
    "    Lakeshore Flood   Z \n",
    "    Lightning    C \n",
    "    Marine Hail    M \n",
    "    Marine High Wind   M \n",
    "    Marine Strong Wind   M \n",
    "    Marine Thunderstorm Wind   M  \n",
    "    Rip Current    Z \n",
    "    Seiche     Z \n",
    "    Sleet      Z \n",
    "    Storm Surge/Tide   Z \n",
    "    Strong Wind    Z \n",
    "    Thunderstorm Wind   C \n",
    "    Tornado    C \n",
    "    Tropical Depression   Z \n",
    "    Tropical Storm   Z \n",
    "    Tsunami    Z \n",
    "    Volcanic Ash   Z \n",
    "    Waterspout    M \n",
    "    Wildfire    Z \n",
    "    Winter Storm   Z \n",
    "    Winter Weather   Z\n",
    "    cz_type  Ex: C, Z , M\n",
    "\n",
    "\n",
    "    cz_fips Ex: 245, 003, 155\n",
    "    The county FIPS number is a unique number assigned to the county by the National Institute for Standards and Technology (NIST) or NWS Forecast Zone Number (See addendum)\n",
    "\n",
    "    cz_name  Ex: AIKEN, RICHMOND, BAXTER (County/Parish, Zone or Marine Name assigned to the county FIPS number or NWS Forecast Zone)\n",
    "\n",
    "    wfo  Ex: CAE, BYZ, GJT (National Weather Service Forecast Office’s area of responsibility (County Warning Area) in which the event occurred)\n",
    "\n",
    "    begin_date_time  Ex: 4/1/2012  20:48\n",
    "    MM/DD/YYYY  24 hour time AM/PM\n",
    "\n",
    "    cz_timezone  Ex: EST-5, MST-7, CST-6\n",
    "    (Time Zone for the County/Parish, Zone or Marine Name)\n",
    "    Eastern Standard Time (EST), Central Standard Time (CST), Mountain Standard Time (MST), etc.\n",
    "\n",
    "    end_date_time  Ex: 4/1/2012  21:03\n",
    "    MM/DD/YYYY  24 hour time AM/PM\n",
    "\n",
    "    injuries_direct  Ex:  1, 0, 56\n",
    "    The number of injuries directly related to the weather event\n",
    "\n",
    "    injuries_indirect  Ex:  0, 15, 87\n",
    "    The number of injuries indirectly related to the weather event\n",
    "\n",
    "    deaths_direct  Ex: 0, 45, 23\n",
    "    The number of deaths directly related to the weather event.\n",
    "\n",
    "    deaths_indirect Ex: 0, 4, 6\n",
    "    The number of deaths indirectly related to the weather event\n",
    "\n",
    "    damage_property  Ex: 10.00K, 0.00K, 10.00M\n",
    "    The estimated amount of damage to property incurred by the weather event.  (e.g. 10.00K = $10,000; 10.00M = $10,000,000)\n",
    "\n",
    "    damage_crops  Ex: 0.00K, 500.00K, 15.00M\n",
    "    The estimated amount of damage to crops incurred by the weather event   (e.g. 10.00K = $10,000; 10.00M = $10,000,000)\n",
    "    source  Ex: Public, Newspaper, Law Enforcement, Broadcast Media, ASOS, Park and Forest Service, Trained Spotter, CoCoRaHS,  etc.  (can be any entry; isn’t restricted in what’s allowed)\n",
    "    Source reporting the weather event\n",
    "\n",
    "    magnitude  Ex: 0.75, 60, 0.88, 2.75\n",
    "    measured extent of the magnitude type ~ only used for wind speeds and hail size (e.g. 0.75” of hail; 60 mph winds)\n",
    "\n",
    "    magnitude_type   Ex: EG, MS, MG, ES\n",
    "    EG = Wind Estimated Gust; ES = Estimated Sustained Wind; MS = Measured Sustained Wind; MG = Measured Wind Gust (no magnitude is included for instances of hail)\n",
    "\n",
    "    flood_cause  Ex:  Ice Jam, Heavy Rain, Heavy Rain/Snow Melt\n",
    "    Reported or estimated cause of the flood\n",
    "\n",
    "    category  Ex: \n",
    "    Unknown (During the time of downloading this particular file, NCDC has never seen anything provided within this field.)\n",
    "\n",
    "    tor_f_scale  Ex: EF0, EF1, EF2, EF3, EF4, EF5\n",
    "    Enhanced Fujita Scale describes the strength of the tornado based on the amount and type of damage caused by the tornado.  The F-scale of damage will vary in the destruction area; therefore, the highest value of the F-scale is recorded for each event. \n",
    "    EF0 – Light Damage (40 – 72 mph)\n",
    "    EF1 – Moderate Damage (73 – 112 mph)\n",
    "    EF2 – Significant damage (113 – 157 mph)\n",
    "    EF3 – Severe Damage (158 – 206 mph)\n",
    "    EF4 – Devastating Damage (207 – 260 mph)\n",
    "    EF5 – Incredible Damage (261 – 318 mph)\n",
    "\n",
    "    tor_length  Ex: 0.66, 1.05, 0.48\n",
    "    Length of the tornado or tornado segment while on the ground (minimal of tenths of miles)\n",
    "\n",
    "    tor_width  Ex:  25, 50, 2640, 10\n",
    "    Width of the tornado or tornado segment while on the ground (in feet)\n",
    "\n",
    "    tor_other_wfo  Ex: DDC, ICT, TOP,OAX\n",
    "    Indicates the continuation of a tornado segment as it crossed from one National Weather Service Forecast Office to another.  The subsequent WFO identifier is provided within this field\n",
    "\n",
    "    tor_other_cz_state   Ex: KS, NE, OK\n",
    "    The two character representation for the state name of the continuing tornado segment as it crossed from one county or zone to another.  The subsequent 2-Letter State ID is provided within this field.\n",
    "\n",
    "    tor_other_cz_fips  Ex: 41, 127, 153\n",
    "    The FIPS number of the county entered by the continuing tornado segment as it crossed from one county to another.  The subsequent FIPS number is provided within this field.\n",
    "\n",
    "    tor_other_cz_name  Ex: DICKINSON, NEMAHA, SARPY\n",
    "    The FIPS name of the county entered by the continuing tornado segment as it crossed from one county to another.  The subsequent county or zone name is provided within this field in ALL CAPS.\n",
    "\n",
    "    episode_title  Ex: Severe weather outbreak on Saturday April 14 in eastern Nebraska\n",
    "    A short description of the episode.  (Short name for the episode itself as determined by NWS.)\n",
    "\n",
    "    episode_narrative  (The episode narrative depicting the general nature and overall activity of the episode.  The narrative is created by NWS.)  Ex: A strong upper level system over the southern Rockies lifted northeast across the plains causing an intense surface low pressure system and attendant warm front to lift into Nebraska.  \n",
    "\n",
    "    event_narrative  (The event narrative provides more specific details of the individual event .  The event narrative is provided by NWS.) Ex:  Heavy rain caused flash flooding across parts of Wilber.  Rainfall of 2 to 3 inches fell across the area. \n",
    "\n",
    "    Storm Data Location File (named event_locations_YYYYMM.csv):\n",
    "    episode_id   Ex: 60904 (ID assigned by NWS to denote the storm episode; links the storm episode with the information within the event details file) An Episode may contain several different events.\n",
    "    event_id     Ex: 364000, 364001, 364002, 364003 (ID assigned by NWS to note a single, small part that goes into a specific storm episode; links the storm episode between the three files downloaded from SPC’s website)\n",
    "    location_index   Ex: 1-x (Number assigned  by NWS to specific locations within the same Storm event. Each event’s sequentially increasing location index number will have a corresponding lat/lon point)\n",
    "    range   Ex: 0.59, 0.69, 4.84, 1.17 {A hydro-meteorological event will be referenced, minimally, to the nearest tenth of a mile, to the geographical center (not from the village/city boundaries or limits) of a particular village/city, airport, or inland lake, providing that the reference point is documented in the Storm Data software location database. }\n",
    "    azimuth  Ex: ENE, NW, WSW, S  {16-point compass direction from a particular village/city, airport, or inland lake, providing that the reference point is documented in the Storm Data software location database of > 130,000 locations.}\n",
    "    location  Ex:  PINELAND,  CENTER, ORRS, RUSK {center from which the range is calculated and the azimuth is determined}\n",
    "\n",
    "    lat  Ex: 31.25, 31.79, 32.76, 31.80\n",
    "    The latitude where the event occurred {rounded to the hundredths in decimal degrees; includes an ‘-‘ if it’s S of the Equator}\n",
    "\n",
    "    lon  Ex: -93.97, -94.18, -94.52, -95.13\n",
    "    The longitude where the event occurred {rounded to the hundredths in decimal degrees; includes an ‘-‘ if it’s W of the Prime Meridian}\n",
    "\n",
    "    Storm Data Fatality File (named event_fatalities_YYYYMM.csv):\n",
    "    fatality_id  Ex: 17582, 17590, 17597, 18222 (ID assigned by NWS to denote the individual fatality that occurred within a storm event)\n",
    "    event_id  Ex: 364302, 365560, 365945, 367330 (ID assigned by NWS to note a single, small part that goes into a specific storm episode; links the storm episode between the three files downloaded from SPC’s website)\n",
    "    fatality_type  Ex: D , I (D = Direct Fatality; I = Indirect Fatality; assignment of this is determined by NWS software; details below are from NWS Directve 10-1605 at http://www.nws.noaa.gov/directives/sym/pd01016005curr.pdf, Section 2.6)\n",
    "        •\tDirect Fatality:  “A direct fatality or injury is defined as a fatality or injury directly attributable to the hydro-meteorological event itself, or impact by airborne/falling/moving debris, i.e., missiles generated by wind, water, ice, lightning, tornado, etc. “\n",
    "        •\tIndirect Fatality:  “Fatalities and injuries, occurring in the vicinity of a hydrometeorological event, or after it has ended, but not directly caused by impact or debris from the event (weather event was a passive entity), are classified as indirect.”\n",
    "\n",
    "    fatality_date  Ex: 4/3/2012  12:00:00 AM, 4/11/2012  12:00:00 AM\n",
    "    MM/DD/YYYY  12 hour time AM/PM\n",
    "\n",
    "    fatality_age  Ex: 38, 25, 69, 54\n",
    "    The age of the fatalities\n",
    "\n",
    "    fatality_sex  Ex: M, F\n",
    "    The gender of the fatalities\n",
    "\n",
    "    fatality_location  Ex: Under Tree, Boating, Vehicle/Towed Trailer\n",
    "\n",
    "    Direct Fatality Location Table\n",
    "\n",
    "    BF Ball Field\n",
    "    BO Boating\n",
    "    BU Business\n",
    "    CA Camping\n",
    "    CH Church\n",
    "    EQ Heavy Equip/Construction\n",
    "    GF Golfing\n",
    "    IW In Water\n",
    "    LS Long Span Roof\n",
    "    MH Mobile/Trailer Home\n",
    "    OT Other/Unknown\n",
    "    OU Outside/Open Areas\n",
    "    PH Permanent Home\n",
    "    PS Permanent Structure\n",
    "    SC School\n",
    "    TE Telephone\n",
    "    UT Under Tree\n",
    "    VE Vehicle and/or Towed Trailer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EDA\n",
    "I am going to explore my data using a variety of tools to reduce the computational lift."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Loading 700MB dataset from my hard drive\n",
    "\n",
    "import pandas as pd\n",
    "df1 = pd.read_csv('jm_cap.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "LabelEncoder().fit_transform(df1['EVENT_TYPE'])\n",
    "df1['EVENT_TYPE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For the purposes of this project, I have over 900,000 rows of data and 62 unique columns.  After some EDA on the data performed earlier, i have been able to identify reduce some features that will allow me to run my models more efficiently without hitting my CPU. \n",
    "\n",
    "## For the primary feature series \"Event Types\", there are 58 different events, ranging from Tornado's, to Seiche's (which if you didn't know is \"a temporary disturbance or oscillation in the water level of a lake or partially enclosed body of water, especially one caused by changes in atmospheric pressure.\".   \n",
    "\n",
    "## I will be running models for top key event types, with the goal of predicting the absolute and probability of costs, damage, fatalities or injuries for selected event types.  \n",
    "\n",
    "## Time to consolidate this feature set. There are far to many subclassifications (e.g. Hurricane, and Hurricane Typhoon, and Winter Weather, Blizzard,  WINTER WEATHER)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#1 Drought Dataset\n",
    "data1=df1[(df1.EVENT_TYPE == 'Heat') | (df1.EVENT_TYPE == 'Excessive Heat') | (df1.EVENT_TYPE == 'Drought')]\n",
    "\n",
    "data1.EVENT_TYPE = data1.EVENT_TYPE.str.replace('Excessive Heat','Drought') \n",
    "data1.EVENT_TYPE = data1.EVENT_TYPE.str.replace('Heat','Drought') \n",
    "print data1.EVENT_TYPE.unique()\n",
    "data1.to_csv('data1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#2 TOrnado Dataset\n",
    "\n",
    "data2=df1[(df1.EVENT_TYPE == 'Tornado') | (df1.EVENT_TYPE == 'Funnel Cloud') ]\n",
    "data2.EVENT_TYPE = data2.EVENT_TYPE.str.replace('Funnel Cloud','Tornado')\n",
    "print data2.EVENT_TYPE.unique()\n",
    "data2.to_csv('data2.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#3 Flood Dataset\n",
    "data3=df1[(df1.EVENT_TYPE == 'Flash Flood') | (df1.EVENT_TYPE == 'Flood') | (df1.EVENT_TYPE == 'Coastal Flood') ]\n",
    "data3.EVENT_TYPE = data3.EVENT_TYPE.str.replace('Flash Flood','Flood')\n",
    "data3.EVENT_TYPE = data3.EVENT_TYPE.str.replace('Coastal Flood','Flood')\n",
    "data3.EVENT_TYPE = data3.EVENT_TYPE.str.replace('Lakeshore Flood','Flood')\n",
    "print data3.EVENT_TYPE.unique()\n",
    "data3.to_csv('data3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#6 Rename Winter Weather\n",
    "\n",
    "data6=df1[(df1.EVENT_TYPE == 'Cold/Wind Chill') | (df1.EVENT_TYPE == 'Avalanche') | (df1.EVENT_TYPE == 'Winter Storm') | \\\n",
    "           (df1.EVENT_TYPE == 'Heavy Snow') | (df1.EVENT_TYPE == 'Extreme Cold/Wind Chill') | (df1.EVENT_TYPE == 'Ice Storm') | \\\n",
    "           (df1.EVENT_TYPE == 'Winter Weather') | (df1.EVENT_TYPE == 'Blizzard') | (df1.EVENT_TYPE == 'WINTER WEATHER') | \\\n",
    "           (df1.EVENT_TYPE == 'Hail') | (df1.EVENT_TYPE == 'Freezing Fog') | (df1.EVENT_TYPE == 'Frost/Freeze') | \\\n",
    "           (df1.EVENT_TYPE == 'Sleet') | (df1.EVENT_TYPE == 'Marine Hail') | (df1.EVENT_TYPE == 'High Snow') | \\\n",
    "           (df1.EVENT_TYPE == 'Lake-Effect Snow') \n",
    "         ]\n",
    "\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Cold/Wind Chill','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Winter Weather','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Avalanche','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Blizzard','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Heavy Snow','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Hail','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Frost/Freeze','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Freezing Fog','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Ice Storm','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Lake-Effect Snow','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Sleet','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Extreme Winter Storm','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('High Snow','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('WINTER WEATHER','Winter Storm')\n",
    "data6.EVENT_TYPE = data6.EVENT_TYPE.str.replace('Marine ','')\n",
    "\n",
    "print data6.EVENT_TYPE.unique()\n",
    "\n",
    "data6.to_csv('data6.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 7 Hurricane Dataset\n",
    "data7=df1[(df1.EVENT_TYPE == 'Hurricane (Typhoon)') | (df1.EVENT_TYPE == 'Hurricane')]\n",
    "data7.EVENT_TYPE= data7.EVENT_TYPE.str.replace(\"Typhoon\",'')\n",
    "data7.EVENT_TYPE= data7.EVENT_TYPE.str.replace(\"(\",'')\n",
    "data7.EVENT_TYPE= data7.EVENT_TYPE.str.replace(\")\",'')\n",
    "data7.EVENT_TYPE= data7.EVENT_TYPE.str.replace(\" \",'')\n",
    "data7.EVENT_TYPE= data7.EVENT_TYPE.str.replace('High Rain','Hurricane')\n",
    "\n",
    "print data7.EVENT_TYPE.unique()\n",
    "data7.to_csv('data7.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 9 Tropical Storm dataset\n",
    "data9=df1[(df1.EVENT_TYPE == 'Tropical Storm') | (df1.EVENT_TYPE == 'Marine Tropical Storm') | (df1.EVENT_TYPE == 'Tropical Depression')]\n",
    "data9.EVENT_TYPE = data9.EVENT_TYPE.str.replace('Marine ','')\n",
    "data9.EVENT_TYPE = data9.EVENT_TYPE.str.replace('Tropical Depression','Tropical Storm')\n",
    "print data9.EVENT_TYPE.unique()\n",
    "data9.to_csv('data9.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 11 High Wind Datatset\n",
    "\n",
    "data11=df1[(df1.EVENT_TYPE == 'Thunderstorm Wind') | (df1.EVENT_TYPE == 'Marine Strong Wind') | (df1.EVENT_TYPE == 'Marine Thunderstorm Wind') | \\\n",
    "           (df1.EVENT_TYPE == 'Marine High Wind') | (df1.EVENT_TYPE == 'Strong Wind') | (df1.EVENT_TYPE == 'Heavy Wind') | \\\n",
    "           (df1.EVENT_TYPE == 'High Wind')]\n",
    "data11.EVENT_TYPE = data11.EVENT_TYPE.str.replace('Marine Strong Wind','High Wind')\n",
    "data11.EVENT_TYPE = data11.EVENT_TYPE.str.replace('Marine High Wind','High Wind')\n",
    "data11.EVENT_TYPE = data11.EVENT_TYPE.str.replace('Marine Thunderstorm Wind','High Wind')\n",
    "data11.EVENT_TYPE = data11.EVENT_TYPE.str.replace('Thunderstorm Wind','High Wind')\n",
    "data11.EVENT_TYPE = data11.EVENT_TYPE.str.replace('Strong Wind','High Wind')\n",
    "data11.EVENT_TYPE = data11.EVENT_TYPE.str.replace('Heavy Wind','High Wind')\n",
    "print data11.EVENT_TYPE.unique()\n",
    "data11.to_csv('data11.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 14 Rip Current Dataset\n",
    "\n",
    "data14=df1[(df1.EVENT_TYPE == 'High Surf') | (df1.EVENT_TYPE == 'Storm Surge/Tide') | (df1.EVENT_TYPE == 'Rip Current')| (df1.EVENT_TYPE == 'Astronomical Low Tide') ]\n",
    "data14.EVENT_TYPE = data14.EVENT_TYPE.str.replace('Storm Surge/Tide','Rip Current')\n",
    "data14.EVENT_TYPE = data14.EVENT_TYPE.str.replace('High Surf','Rip Current')\n",
    "data14.EVENT_TYPE = data14.EVENT_TYPE.str.replace('Astronomical Low Tide','Rip Current')\n",
    "print data14.EVENT_TYPE.unique()\n",
    "data14.to_csv('data14.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now I am going to clean up my 600,000 row dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Excessive Heat','Drought') \n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Heat','Drought') \n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Funnel Cloud','Tornado')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Flash Flood','Flood')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Coastal Flood','Flood')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Lakeshore Flood','Flood')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Cold/Wind Chill','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Winter Weather','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Avalanche','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Blizzard','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Heavy Snow','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Hail','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Frost/Freeze','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Freezing Fog','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Ice Storm','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Lake-Effect Snow','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Sleet','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Extreme Winter Storm','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('High Snow','Winter Storm')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('WINTER WEATHER','Winter Storm')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Dust Devil','Dust Storm')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Marine ','')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Tropical Depression','Tropical Storm')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Marine Strong Wind','High Wind')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Marine High Wind','High Wind')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Marine Thunderstorm Wind','High Wind')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Thunderstorm Wind','High Wind')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Strong Wind','High Wind')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Heavy Wind','High Wind')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Seiche','Flood')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Storm Surge/Tide','Rip Current')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('High Surf','Rip Current')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Astronomical Low Tide','Rip Current')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Sneakerwave','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Debris Flow','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Dense Smoke','Wildfire')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('()','')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Volcanic Ash','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Dust Storm','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Dust Devil','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Northern Lights','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Debris Flow','OTHER')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Dense Fog','OTHER')\n",
    "\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace(\"Typhoon\",'')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace(\"(\",'')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace(\")\",'')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('High Rain','Hurricane')\n",
    "df1.EVENT_TYPE = df1.EVENT_TYPE.str.replace('Hurricane ','Hurricane')\n",
    "\n",
    "print df1.EVENT_TYPE.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabelEncoder().fit_transform(df1['STATE'])\n",
    "df1['STATE'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.TOR_F_SCALE = df1.TOR_F_SCALE.apply(lambda x: str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['TOR_F_SCALE'] = df1['TOR_F_SCALE'].apply(lambda x: x.replace(' ', ''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "LabelEncoder().fit_transform(df1['TOR_F_SCALE'])\n",
    "df1['TOR_F_SCALE'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Now going to do some further EDA on my EVENT_TYPE datasets to see of crop/property damage, casualities and fatalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').INJURIES_DIRECT.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').INJURIES_INDIRECT.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').DEATHS_DIRECT.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').DEATHS_INDIRECT.agg(['count','min', 'max', 'mean', 'sum']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Total Casualities from Tornados\n",
    "torid1= df1[df1.EVENT_TYPE =='Tornado'].DEATHS_INDIRECT.sum()\n",
    "torid2= df1[df1.EVENT_TYPE =='Tornado'].DEATHS_DIRECT.sum()\n",
    "torid3= df1[df1.EVENT_TYPE =='Tornado'].INJURIES_DIRECT.sum()\n",
    "torid4= df1[df1.EVENT_TYPE =='Tornado'].INJURIES_INDIRECT.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TORSUM = torid1 + torid2 + torid3 + torid4\n",
    "print TORSUM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Want to add a column with a combination of injuries and deaths called casualties\n",
    "df1['CASUALTIES'] = pd.DataFrame(df1.INJURIES_DIRECT + df1.INJURIES_INDIRECT + df1.DEATHS_DIRECT + df1.DEATHS_INDIRECT)\n",
    "df1['TOTAL_DEATHS'] = pd.DataFrame(df1.DEATHS_DIRECT + df1.DEATHS_INDIRECT)\n",
    "df1['TOTAL_INJURIES'] = pd.DataFrame(df1.INJURIES_DIRECT + df1.INJURIES_INDIRECT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').CASUALTIES.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame(df1.groupby('EVENT_TYPE').CASUALTIES.sum()).sort_values('CASUALTIES', axis=0, ascending=False)\n",
    "a = a[a['CASUALTIES'] > 100 ]\n",
    "\n",
    "d = pd.DataFrame(df1.groupby('EVENT_TYPE').TOTAL_DEATHS.sum()).sort_values('TOTAL_DEATHS', axis=0, ascending=False)\n",
    "d = d[d['TOTAL_DEATHS'] > 100 ]\n",
    "\n",
    "i = pd.DataFrame(df1.groupby('EVENT_TYPE').TOTAL_INJURIES.sum()).sort_values('TOTAL_INJURIES', axis=0, ascending=False)\n",
    "i = i[i['TOTAL_INJURIES'] > 100 ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a.plot(kind='bar', color = 'Red',figsize=(20,7))\n",
    "i.plot(kind='bar',figsize=(20,7))\n",
    "d.plot(kind='bar',color = 'green',figsize=(20,7))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Cleaning....."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# I may need to combine a few dataSeries to get a date\n",
    "df1['DATE'] = pd.to_datetime(df1.BEGIN_YEARMONTH*100+df1.BEGIN_DAY,format='%Y%m%d',errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def value_to_float(x):\n",
    "    if type(x) == float or type(x) == int:\n",
    "        return x\n",
    "    if 'K' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('K', '')) * 1000\n",
    "        return 1000.0\n",
    "    if 'M' in x:\n",
    "        if len(x) > 1:\n",
    "            return float(x.replace('M', '')) * 1000000\n",
    "        return 1000000.0\n",
    "    if 'B' in x:\n",
    "        return float(x.replace('B', '')) * 1000000000\n",
    "    return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.DAMAGE_CROPS.fillna(value=0.0, inplace=True)\n",
    "df1.DAMAGE_CROPS = df1.DAMAGE_CROPS.apply(value_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.DAMAGE_PROPERTY.fillna(value=0.0, inplace=True)\n",
    "df1.DAMAGE_PROPERTY = df1.DAMAGE_PROPERTY.apply(value_to_float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').DAMAGE_CROPS.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df1.groupby('EVENT_TYPE').DAMAGE_PROPERTY.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1['COMBINED_DAMAGE'] = pd.DataFrame(df1.DAMAGE_PROPERTY + df1.DAMAGE_CROPS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.groupby('EPISODE_ID').COMBINED_DAMAGE.agg(['count','min', 'max', 'mean', 'sum']).sort_values('sum', axis=0, ascending=False).reset_index().head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "b = pd.DataFrame(df1.groupby('EVENT_TYPE').COMBINED_DAMAGE.sum()).sort_values('COMBINED_DAMAGE', axis=0, ascending=False)\n",
    "b = b[b['COMBINED_DAMAGE'] > 100000000]\n",
    "\n",
    "m = pd.DataFrame(df1.groupby('EVENT_TYPE').DAMAGE_PROPERTY.sum()).sort_values('DAMAGE_PROPERTY', axis=0, ascending=False)\n",
    "m = m[m['DAMAGE_PROPERTY'] > 100 ]\n",
    "\n",
    "q = pd.DataFrame(df1.groupby('EVENT_TYPE').DAMAGE_CROPS.sum()).sort_values('DAMAGE_CROPS', axis=0, ascending=False)\n",
    "q = q[q['DAMAGE_CROPS'] > 100 ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b.plot(kind='bar', color = 'Red',figsize=(20,7))\n",
    "m.plot(kind='bar',figsize=(20,7))\n",
    "q.plot(kind='bar',color = 'green',figsize=(20,7))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.drop(['BEGIN_YEARMONTH','BEGIN_TIME', 'END_YEARMONTH', 'END_DAY', 'END_TIME', 'STATE_FIPS',\n",
    "         'CZ_TYPE','CZ_FIPS', 'BEGIN_DATE_TIME', 'CZ_TIMEZONE', 'END_DATE_TIME', 'SOURCE', 'MAGNITUDE', 'MAGNITUDE_TYPE',\n",
    "         'TOR_OTHER_CZ_FIPS', 'TOR_OTHER_CZ_NAME', 'BEGIN_RANGE', 'BEGIN_AZIMUTH', 'BEGIN_LOCATION', 'END_RANGE', 'END_AZIMUTH',\n",
    "          'END_LOCATION', 'BEGIN_LAT','BEGIN_LON' , 'END_LAT', 'END_LON','LAST_MOD_DATE', 'LAST_MOD_TIME', 'LAST_CERT_DATE', 'LAST_CERT_TIME',\n",
    "          'LAST_MOD', 'LAST_CERT', 'ADDCORR_FLG','ADDCORR_DATE','BEGIN_DAY', 'WFO', 'FLOOD_CAUSE','TOR_OTHER_WFO','TOR_OTHER_CZ_STATE'\n",
    "         ], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cols = ['EVENT_TYPE','EVENT_ID','EPISODE_ID','DATE','CZ_NAME','STATE','INJURIES_DIRECT','INJURIES_INDIRECT','TOTAL_INJURIES','DEATHS_DIRECT','DEATHS_INDIRECT','TOTAL_DEATHS','CASUALTIES','DAMAGE_PROPERTY','DAMAGE_CROPS','COMBINED_DAMAGE','CATEGORY','TOR_F_SCALE','TOR_LENGTH','TOR_WIDTH','EPISODE_NARRATIVE','EVENT_NARRATIVE','YEAR','MONTH_NAME']\n",
    "df1.columns = cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#convert dataframe to csv\n",
    "\n",
    "df1.to_csv('df2.csv')\n",
    "#This worked!  Thank God this was successful on Tuesday school!\n",
    "# 10 years worth of data (654313, 24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus: Explain how you intend to evaluate your results. \n",
    "What tuning metric and evaluation approaches do you intend to use?\n",
    "Clean> fit> run models, look at the results... \n",
    "\n",
    "Linear regression: Attempt to accurately predict the independent variable (casualities) by good fitting SSE / differential \n",
    "\n",
    "Logistic regression: Attempt to predict if I can say if a certain storm in a certain state, in a certain month would cause a causality. I would show classification accuracy score, confusion matrix coorelation number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Bonus Identify 1-2 additional datasets that may help you triangulate your findings. \n",
    "How might these relate to your data?\n",
    "\n",
    "Could look at the aircraft incident reports from the NTSB site\n",
    "May get an idea of crop damage in current dollar basis, perhaps rebase for inflation or the proper crop price in todays dollars, crop price website"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful Resources\n",
    "\n",
    "- [Exploratory Data Analysis](http://insightdatascience.com/blog/eda-and-graphics-eli-bressert.html)\n",
    "- [Best practices for data documentation](https://www.dataone.org/all-best-practices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
